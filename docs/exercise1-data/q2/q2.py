import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from sklearn.decomposition import PCA
from sklearn.linear_model import LogisticRegression
from sklearn.neural_network import MLPClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report
import os

# Configurar seed para reprodutibilidade
np.random.seed(42)

def generate_5d_dataset():
    """
    Gera um dataset 5D com duas classes usando distribui√ß√µes normais multivariadas.
    
    Especifica√ß√µes:
    - 500 amostras para Classe A
    - 500 amostras para Classe B
    - 5 dimens√µes por amostra
    - Total: 1000 amostras
    """
    
    print("üîß Gerando dataset 5D com duas classes...")
    
    # Par√¢metros para Classe A
    mean_A = np.array([0, 0, 0, 0, 0])
    cov_A = np.array([
        [1.0, 0.8, 0.1, 0.0, 0.0],
        [0.8, 1.0, 0.3, 0.0, 0.0],
        [0.1, 0.3, 1.0, 0.5, 0.0],
        [0.0, 0.0, 0.5, 1.0, 0.2],
        [0.0, 0.0, 0.0, 0.2, 1.0]
    ])
    
    # Par√¢metros para Classe B
    mean_B = np.array([1.5, 1.5, 1.5, 1.5, 1.5])
    cov_B = np.array([
        [1.5, -0.7, 0.2, 0.0, 0.0],
        [-0.7, 1.5, 0.4, 0.0, 0.0],
        [0.2, 0.4, 1.5, 0.6, 0.0],
        [0.0, 0.0, 0.6, 1.5, 0.3],
        [0.0, 0.0, 0.0, 0.3, 1.5]
    ])
    
    # Gerar amostras para Classe A
    samples_A = np.random.multivariate_normal(mean_A, cov_A, 500)
    labels_A = np.zeros(500, dtype=int)  # Classe A = 0
    
    # Gerar amostras para Classe B
    samples_B = np.random.multivariate_normal(mean_B, cov_B, 500)
    labels_B = np.ones(500, dtype=int)   # Classe B = 1
    
    # Combinar dados
    X = np.vstack([samples_A, samples_B])
    y = np.hstack([labels_A, labels_B])
    
    print(f"‚úÖ Dataset 5D gerado com sucesso!")
    print(f"üìä Forma dos dados: {X.shape}")
    print(f"üè∑Ô∏è  Classes: {np.unique(y)}")
    print(f"üìà Amostras por classe: {np.bincount(y)}")
    
    # Mostrar estat√≠sticas das classes
    print(f"\nüìã Estat√≠sticas Classe A:")
    print(f"   M√©dia real: {np.mean(samples_A, axis=0)}")
    print(f"   Desvio padr√£o: {np.std(samples_A, axis=0)}")
    
    print(f"\nüìã Estat√≠sticas Classe B:")
    print(f"   M√©dia real: {np.mean(samples_B, axis=0)}")
    print(f"   Desvio padr√£o: {np.std(samples_B, axis=0)}")
    
    return X, y, samples_A, samples_B

def apply_pca_analysis(X, y):
    """
    Aplica PCA para reduzir dimensionalidade de 5D para 2D e analisa a vari√¢ncia.
    """
    
    print("\n" + "="*60)
    print("üîç AN√ÅLISE DE COMPONENTES PRINCIPAIS (PCA)")
    print("="*60)
    
    # Aplicar PCA
    pca = PCA()
    X_pca = pca.fit_transform(X)
    
    # Mostrar vari√¢ncia explicada
    explained_variance_ratio = pca.explained_variance_ratio_
    cumulative_variance = np.cumsum(explained_variance_ratio)
    
    print("üìä Vari√¢ncia explicada por componente:")
    for i, (var, cum_var) in enumerate(zip(explained_variance_ratio, cumulative_variance)):
        print(f"   PC{i+1}: {var:.3f} ({var*100:.1f}%) - Cumulativa: {cum_var:.3f} ({cum_var*100:.1f}%)")
    
    # Reduzir para 2D
    pca_2d = PCA(n_components=2)
    X_2d = pca_2d.fit_transform(X)
    
    print(f"\nüéØ Redu√ß√£o para 2D:")
    print(f"   Vari√¢ncia explicada pelos 2 primeiros PCs: {pca_2d.explained_variance_ratio_.sum():.3f} ({pca_2d.explained_variance_ratio_.sum()*100:.1f}%)")
    print(f"   PC1: {pca_2d.explained_variance_ratio_[0]:.3f} ({pca_2d.explained_variance_ratio_[0]*100:.1f}%)")
    print(f"   PC2: {pca_2d.explained_variance_ratio_[1]:.3f} ({pca_2d.explained_variance_ratio_[1]*100:.1f}%)")
    
    return X_2d, pca_2d, explained_variance_ratio

def visualize_2d_projection(X_2d, y):
    """
    Cria visualiza√ß√£o 2D do dataset ap√≥s redu√ß√£o dimensional.
    """
    
    print("\n" + "="*60)
    print("üé® VISUALIZA√á√ÉO 2D DO DATASET")
    print("="*60)
    
    # Configurar cores e nomes
    colors = ['red', 'blue']
    class_names = ['Classe A', 'Classe B']
    
    # Criar gr√°fico
    plt.figure(figsize=(12, 8))
    
    for class_id in range(2):
        mask = y == class_id
        class_data = X_2d[mask]
        
        plt.scatter(class_data[:, 0], class_data[:, 1], 
                   c=colors[class_id], label=class_names[class_id], 
                   alpha=0.7, s=50, edgecolors='black', linewidth=0.5)
    
    plt.xlabel('Primeira Componente Principal (PC1)')
    plt.ylabel('Segunda Componente Principal (PC2)')
    plt.title('Dataset 5D Projetado em 2D usando PCA\n(1000 amostras, 2 classes)')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # Salvar gr√°fico
    script_dir = os.path.dirname(os.path.abspath(__file__))
    output_path = os.path.join(script_dir, 'dataset_5d_2d_projection.png')
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    print(f"üìä Gr√°fico salvo como 'dataset_5d_2d_projection.png'")
    
    plt.show()
    
    return X_2d

def analyze_linear_separability(X, y, X_2d):
    """
    Analisa a separabilidade linear dos dados em 5D e 2D.
    """
    
    print("\n" + "="*60)
    print("üîç AN√ÅLISE DE SEPARABILIDADE LINEAR")
    print("="*60)
    
    # Testar separabilidade em 5D
    print("üìä Testando separabilidade em 5D:")
    lr_5d = LogisticRegression(max_iter=1000, random_state=42)
    lr_5d.fit(X, y)
    score_5d = lr_5d.score(X, y)
    print(f"   Acur√°cia Regress√£o Log√≠stica (5D): {score_5d:.3f}")
    
    # Testar separabilidade em 2D
    print("\nüìä Testando separabilidade em 2D (ap√≥s PCA):")
    lr_2d = LogisticRegression(max_iter=1000, random_state=42)
    lr_2d.fit(X_2d, y)
    score_2d = lr_2d.score(X_2d, y)
    print(f"   Acur√°cia Regress√£o Log√≠stica (2D): {score_2d:.3f}")
    
    # An√°lise da separabilidade
    print(f"\nüéØ An√°lise da Separabilidade:")
    if score_5d > 0.95:
        print("   ‚úÖ Dados 5D s√£o LINEARMENTE SEPAR√ÅVEIS")
    elif score_5d > 0.8:
        print("   ‚ö†Ô∏è  Dados 5D s√£o PARCIALMENTE LINEARMENTE SEPAR√ÅVEIS")
    else:
        print("   ‚ùå Dados 5D N√ÉO s√£o linearmente separ√°veis")
    
    if score_2d > 0.95:
        print("   ‚úÖ Proje√ß√£o 2D √© LINEARMENTE SEPAR√ÅVEL")
    elif score_2d > 0.8:
        print("   ‚ö†Ô∏è  Proje√ß√£o 2D √© PARCIALMENTE LINEARMENTE SEPAR√ÅVEL")
    else:
        print("   ‚ùå Proje√ß√£o 2D N√ÉO √© linearmente separ√°vel")
    
    return score_5d, score_2d

def test_neural_networks(X, y):
    """
    Testa diferentes arquiteturas de redes neurais.
    """
    
    print("\n" + "="*60)
    print("üß† TESTE DE REDES NEURAIS")
    print("="*60)
    
    # Dividir dados
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    
    # Configura√ß√µes de redes neurais
    nn_configs = [
        (MLPClassifier(hidden_layer_sizes=(10,), max_iter=1000, random_state=42), "NN Simples (10)"),
        (MLPClassifier(hidden_layer_sizes=(50,), max_iter=1000, random_state=42), "NN M√©dia (50)"),
        (MLPClassifier(hidden_layer_sizes=(100,), max_iter=1000, random_state=42), "NN Grande (100)"),
        (MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=1000, random_state=42), "NN Profunda (100-50)"),
        (MLPClassifier(hidden_layer_sizes=(200, 100, 50), max_iter=1000, random_state=42), "NN Muito Profunda (200-100-50)")
    ]
    
    print("üî¨ Testando diferentes arquiteturas:")
    results = []
    
    for model, name in nn_configs:
        model.fit(X_train, y_train)
        train_score = model.score(X_train, y_train)
        test_score = model.score(X_test, y_test)
        results.append((name, train_score, test_score))
        print(f"   {name}:")
        print(f"      Treino: {train_score:.3f}")
        print(f"      Teste:  {test_score:.3f}")
    
    return results

def create_decision_boundary_plot_2d(X_2d, y):
    """
    Cria gr√°fico com fronteiras de decis√£o em 2D.
    """
    
    print("\n" + "="*60)
    print("üé® FRONTEIRAS DE DECIS√ÉO EM 2D")
    print("="*60)
    
    # Configurar a grade
    h = 0.1
    x_min, x_max = X_2d[:, 0].min() - 1, X_2d[:, 0].max() + 1
    y_min, y_max = X_2d[:, 1].min() - 1, X_2d[:, 1].max() + 1
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                         np.arange(y_min, y_max, h))
    
    # Modelos para testar
    models = [
        LogisticRegression(max_iter=1000, random_state=42),
        MLPClassifier(hidden_layer_sizes=(50,), max_iter=1000, random_state=42),
        MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=1000, random_state=42)
    ]
    
    model_names = [
        "Regress√£o Log√≠stica (Linear)",
        "Rede Neural Simples (50)",
        "Rede Neural Profunda (100-50)"
    ]
    
    # Criar subplots
    fig, axes = plt.subplots(1, 3, figsize=(18, 5))
    colors = ['red', 'blue']
    class_names = ['Classe A', 'Classe B']
    
    for idx, (model, name) in enumerate(zip(models, model_names)):
        ax = axes[idx]
        
        # Treinar modelo
        model.fit(X_2d, y)
        
        # Fazer predi√ß√µes na grade
        Z = model.predict(np.c_[xx.ravel(), yy.ravel()])
        Z = Z.reshape(xx.shape)
        
        # Plotar regi√µes de decis√£o
        ax.contourf(xx, yy, Z, alpha=0.3, colors=colors)
        
        # Plotar pontos
        for class_id in range(2):
            mask = y == class_id
            class_data = X_2d[mask]
            ax.scatter(class_data[:, 0], class_data[:, 1], 
                      c=colors[class_id], label=class_names[class_id], 
                      alpha=0.7, s=30, edgecolors='black', linewidth=0.5)
        
        # Configurar gr√°fico
        ax.set_xlim(x_min, x_max)
        ax.set_ylim(y_min, y_max)
        ax.set_xlabel('PC1')
        ax.set_ylabel('PC2')
        ax.set_title(f'{name}\nAcur√°cia: {model.score(X_2d, y):.3f}')
        ax.legend()
        ax.grid(True, alpha=0.3)
    
    plt.tight_layout()
    
    # Salvar gr√°fico
    script_dir = os.path.dirname(os.path.abspath(__file__))
    output_path = os.path.join(script_dir, 'decision_boundaries_2d.png')
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    print(f"üìä Gr√°fico de fronteiras salvo como 'decision_boundaries_2d.png'")
    
    plt.show()

def save_dataset_5d(X, y):
    """
    Salva o dataset 5D em arquivo CSV.
    """
    
    # Criar DataFrame
    df = pd.DataFrame(X, columns=[f'feature_{i+1}' for i in range(5)])
    df['class'] = y
    df['class_name'] = df['class'].map({0: 'A', 1: 'B'})
    
    # Salvar CSV
    script_dir = os.path.dirname(os.path.abspath(__file__))
    output_path = os.path.join(script_dir, 'dataset_5d.csv')
    df.to_csv(output_path, index=False)
    print(f"üíæ Dataset 5D salvo como 'dataset_5d.csv'")
    
    return df


def main():
    """
    Fun√ß√£o principal que executa todo o pipeline do Exerc√≠cio 2.
    """
    
    print("üöÄ EXERC√çCIO 2: N√ÉO-LINEARIDADE EM DIMENS√ïES ALTAS")
    print("="*60)
    
    # Gerar dataset 5D usando NumPy
    print("\nüîµ Gerando dataset 5D com NumPy...")
    X, y, samples_A, samples_B = generate_5d_dataset()
    X_2d, pca_2d, explained_variance = apply_pca_analysis(X, y)
    
    # Visualizar proje√ß√£o 2D
    print("\nüé® Visualizando proje√ß√£o 2D...")
    visualize_2d_projection(X_2d, y)
    
    # Analisar separabilidade linear
    print("\nüîç Analisando separabilidade linear...")
    score_5d, score_2d = analyze_linear_separability(X, y, X_2d)
    
    # Testar redes neurais
    print("\nüß† Testando redes neurais...")
    nn_results = test_neural_networks(X, y)
    
    # Criar gr√°ficos de fronteiras de decis√£o
    print("\nüé® Criando fronteiras de decis√£o...")
    create_decision_boundary_plot_2d(X_2d, y)
    
    # Salvar dataset
    print("\nüíæ Salvando dataset...")
    df = save_dataset_5d(X, y)
    
    # Resumo final
    print("\n" + "="*60)
    print("üìã RESUMO DO EXERC√çCIO 2")
    print("="*60)
    
    print("üéØ Dataset gerado:")
    print("   ‚Ä¢ 1000 amostras (500 por classe)")
    print("   ‚Ä¢ 5 dimens√µes por amostra")
    print("   ‚Ä¢ 2 classes (A e B)")
    
    print(f"\nüìä An√°lise PCA:")
    print(f"   ‚Ä¢ Vari√¢ncia explicada pelos 2 primeiros PCs: {pca_2d.explained_variance_ratio_.sum():.1%}")
    print(f"   ‚Ä¢ PC1: {pca_2d.explained_variance_ratio_[0]:.1%}")
    print(f"   ‚Ä¢ PC2: {pca_2d.explained_variance_ratio_[1]:.1%}")
    
    print(f"\nüîç Separabilidade Linear:")
    print(f"   ‚Ä¢ 5D: {score_5d:.3f}")
    print(f"   ‚Ä¢ 2D: {score_2d:.3f}")
    
    print(f"\nüß† Melhor Rede Neural:")
    best_nn = max(nn_results, key=lambda x: x[2])
    print(f"   ‚Ä¢ {best_nn[0]}: {best_nn[2]:.3f}")
    
    print(f"\nüìÅ Arquivos gerados:")
    print(f"   ‚Ä¢ dataset_5d.csv")
    print(f"   ‚Ä¢ dataset_5d_2d_projection.png")
    print(f"   ‚Ä¢ decision_boundaries_2d.png")
    
    return X, y, X_2d, df

if __name__ == "__main__":
    X, y, X_2d, df = main()
