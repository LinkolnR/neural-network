import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from sklearn.decomposition import PCA
from sklearn.linear_model import LogisticRegression
from sklearn.neural_network import MLPClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report
import os

# Configurar seed para reprodutibilidade
np.random.seed(42)

def generate_5d_dataset():
    """
    Gera um dataset 5D com duas classes usando distribuiÃ§Ãµes normais multivariadas.
    
    EspecificaÃ§Ãµes:
    - 500 amostras para Classe A
    - 500 amostras para Classe B
    - 5 dimensÃµes por amostra
    - Total: 1000 amostras
    """
    
    print("ğŸ”§ Gerando dataset 5D com duas classes...")
    
    # ParÃ¢metros para Classe A
    mean_A = np.array([0, 0, 0, 0, 0])
    cov_A = np.array([
        [1.0, 0.8, 0.1, 0.0, 0.0],
        [0.8, 1.0, 0.3, 0.0, 0.0],
        [0.1, 0.3, 1.0, 0.5, 0.0],
        [0.0, 0.0, 0.5, 1.0, 0.2],
        [0.0, 0.0, 0.0, 0.2, 1.0]
    ])
    
    # ParÃ¢metros para Classe B
    mean_B = np.array([1.5, 1.5, 1.5, 1.5, 1.5])
    cov_B = np.array([
        [1.5, -0.7, 0.2, 0.0, 0.0],
        [-0.7, 1.5, 0.4, 0.0, 0.0],
        [0.2, 0.4, 1.5, 0.6, 0.0],
        [0.0, 0.0, 0.6, 1.5, 0.3],
        [0.0, 0.0, 0.0, 0.3, 1.5]
    ])
    
    # Gerar amostras para Classe A
    samples_A = np.random.multivariate_normal(mean_A, cov_A, 500)
    labels_A = np.zeros(500, dtype=int)  # Classe A = 0
    
    # Gerar amostras para Classe B
    samples_B = np.random.multivariate_normal(mean_B, cov_B, 500)
    labels_B = np.ones(500, dtype=int)   # Classe B = 1
    
    # Combinar dados
    X = np.vstack([samples_A, samples_B])
    y = np.hstack([labels_A, labels_B])
    
    print(f"âœ… Dataset 5D gerado com sucesso!")
    print(f"ğŸ“Š Forma dos dados: {X.shape}")
    print(f"ğŸ·ï¸  Classes: {np.unique(y)}")
    print(f"ğŸ“ˆ Amostras por classe: {np.bincount(y)}")
    
    # Mostrar estatÃ­sticas das classes
    print(f"\nğŸ“‹ EstatÃ­sticas Classe A:")
    print(f"   MÃ©dia real: {np.mean(samples_A, axis=0)}")
    print(f"   Desvio padrÃ£o: {np.std(samples_A, axis=0)}")
    
    print(f"\nğŸ“‹ EstatÃ­sticas Classe B:")
    print(f"   MÃ©dia real: {np.mean(samples_B, axis=0)}")
    print(f"   Desvio padrÃ£o: {np.std(samples_B, axis=0)}")
    
    return X, y, samples_A, samples_B

def apply_pca_analysis(X, y):
    """
    Aplica PCA para reduzir dimensionalidade de 5D para 2D e analisa a variÃ¢ncia.
    """
    
    print("\n" + "="*60)
    print("ğŸ” ANÃLISE DE COMPONENTES PRINCIPAIS (PCA)")
    print("="*60)
    
    # Aplicar PCA
    pca = PCA()
    X_pca = pca.fit_transform(X)
    
    # Mostrar variÃ¢ncia explicada
    explained_variance_ratio = pca.explained_variance_ratio_
    cumulative_variance = np.cumsum(explained_variance_ratio)
    
    print("ğŸ“Š VariÃ¢ncia explicada por componente:")
    for i, (var, cum_var) in enumerate(zip(explained_variance_ratio, cumulative_variance)):
        print(f"   PC{i+1}: {var:.3f} ({var*100:.1f}%) - Cumulativa: {cum_var:.3f} ({cum_var*100:.1f}%)")
    
    # Reduzir para 2D
    pca_2d = PCA(n_components=2)
    X_2d = pca_2d.fit_transform(X)
    
    print(f"\nğŸ¯ ReduÃ§Ã£o para 2D:")
    print(f"   VariÃ¢ncia explicada pelos 2 primeiros PCs: {pca_2d.explained_variance_ratio_.sum():.3f} ({pca_2d.explained_variance_ratio_.sum()*100:.1f}%)")
    print(f"   PC1: {pca_2d.explained_variance_ratio_[0]:.3f} ({pca_2d.explained_variance_ratio_[0]*100:.1f}%)")
    print(f"   PC2: {pca_2d.explained_variance_ratio_[1]:.3f} ({pca_2d.explained_variance_ratio_[1]*100:.1f}%)")
    
    return X_2d, pca_2d, explained_variance_ratio

def visualize_2d_projection(X_2d, y):
    """
    Cria visualizaÃ§Ã£o 2D do dataset apÃ³s reduÃ§Ã£o dimensional.
    """
    
    print("\n" + "="*60)
    print("ğŸ¨ VISUALIZAÃ‡ÃƒO 2D DO DATASET")
    print("="*60)
    
    # Configurar cores e nomes
    colors = ['red', 'blue']
    class_names = ['Classe A', 'Classe B']
    
    # Criar grÃ¡fico
    plt.figure(figsize=(12, 8))
    
    for class_id in range(2):
        mask = y == class_id
        class_data = X_2d[mask]
        
        plt.scatter(class_data[:, 0], class_data[:, 1], 
                   c=colors[class_id], label=class_names[class_id], 
                   alpha=0.7, s=50, edgecolors='black', linewidth=0.5)
    
    plt.xlabel('Primeira Componente Principal (PC1)')
    plt.ylabel('Segunda Componente Principal (PC2)')
    plt.title('Dataset 5D Projetado em 2D usando PCA\n(1000 amostras, 2 classes)')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # Salvar grÃ¡fico
    script_dir = os.path.dirname(os.path.abspath(__file__))
    output_path = os.path.join(script_dir, 'dataset_5d_2d_projection.png')
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    print(f"ğŸ“Š GrÃ¡fico salvo como 'dataset_5d_2d_projection.png'")
    
    plt.show()
    
    return X_2d

def analyze_linear_separability(X, y, X_2d):
    """
    Analisa a separabilidade linear dos dados em 5D e 2D.
    """
    
    print("\n" + "="*60)
    print("ğŸ” ANÃLISE DE SEPARABILIDADE LINEAR")
    print("="*60)
    
    # Testar separabilidade em 5D
    print("ğŸ“Š Testando separabilidade em 5D:")
    lr_5d = LogisticRegression(max_iter=1000, random_state=42)
    lr_5d.fit(X, y)
    score_5d = lr_5d.score(X, y)
    print(f"   AcurÃ¡cia RegressÃ£o LogÃ­stica (5D): {score_5d:.3f}")
    
    # Testar separabilidade em 2D
    print("\nğŸ“Š Testando separabilidade em 2D (apÃ³s PCA):")
    lr_2d = LogisticRegression(max_iter=1000, random_state=42)
    lr_2d.fit(X_2d, y)
    score_2d = lr_2d.score(X_2d, y)
    print(f"   AcurÃ¡cia RegressÃ£o LogÃ­stica (2D): {score_2d:.3f}")
    
    # AnÃ¡lise da separabilidade
    print(f"\nğŸ¯ AnÃ¡lise da Separabilidade:")
    if score_5d > 0.95:
        print("   âœ… Dados 5D sÃ£o LINEARMENTE SEPARÃVEIS")
    elif score_5d > 0.8:
        print("   âš ï¸  Dados 5D sÃ£o PARCIALMENTE LINEARMENTE SEPARÃVEIS")
    else:
        print("   âŒ Dados 5D NÃƒO sÃ£o linearmente separÃ¡veis")
    
    if score_2d > 0.95:
        print("   âœ… ProjeÃ§Ã£o 2D Ã© LINEARMENTE SEPARÃVEL")
    elif score_2d > 0.8:
        print("   âš ï¸  ProjeÃ§Ã£o 2D Ã© PARCIALMENTE LINEARMENTE SEPARÃVEL")
    else:
        print("   âŒ ProjeÃ§Ã£o 2D NÃƒO Ã© linearmente separÃ¡vel")
    
    return score_5d, score_2d

def test_neural_networks(X, y):
    """
    Testa diferentes arquiteturas de redes neurais.
    """
    
    print("\n" + "="*60)
    print("ğŸ§  TESTE DE REDES NEURAIS")
    print("="*60)
    
    # Dividir dados
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    
    # ConfiguraÃ§Ãµes de redes neurais
    nn_configs = [
        (MLPClassifier(hidden_layer_sizes=(10,), max_iter=1000, random_state=42), "NN Simples (10)"),
        (MLPClassifier(hidden_layer_sizes=(50,), max_iter=1000, random_state=42), "NN MÃ©dia (50)"),
        (MLPClassifier(hidden_layer_sizes=(100,), max_iter=1000, random_state=42), "NN Grande (100)"),
        (MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=1000, random_state=42), "NN Profunda (100-50)"),
        (MLPClassifier(hidden_layer_sizes=(200, 100, 50), max_iter=1000, random_state=42), "NN Muito Profunda (200-100-50)")
    ]
    
    print("ğŸ”¬ Testando diferentes arquiteturas:")
    results = []
    
    for model, name in nn_configs:
        model.fit(X_train, y_train)
        train_score = model.score(X_train, y_train)
        test_score = model.score(X_test, y_test)
        results.append((name, train_score, test_score))
        print(f"   {name}:")
        print(f"      Treino: {train_score:.3f}")
        print(f"      Teste:  {test_score:.3f}")
    
    return results

def create_decision_boundary_plot_2d(X_2d, y):
    """
    Cria grÃ¡fico com fronteiras de decisÃ£o em 2D.
    """
    
    print("\n" + "="*60)
    print("ğŸ¨ FRONTEIRAS DE DECISÃƒO EM 2D")
    print("="*60)
    
    # Configurar a grade
    h = 0.1
    x_min, x_max = X_2d[:, 0].min() - 1, X_2d[:, 0].max() + 1
    y_min, y_max = X_2d[:, 1].min() - 1, X_2d[:, 1].max() + 1
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                         np.arange(y_min, y_max, h))
    
    # Modelos para testar
    models = [
        LogisticRegression(max_iter=1000, random_state=42),
        MLPClassifier(hidden_layer_sizes=(50,), max_iter=1000, random_state=42),
        MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=1000, random_state=42)
    ]
    
    model_names = [
        "RegressÃ£o LogÃ­stica (Linear)",
        "Rede Neural Simples (50)",
        "Rede Neural Profunda (100-50)"
    ]
    
    # Criar subplots
    fig, axes = plt.subplots(1, 3, figsize=(18, 5))
    colors = ['red', 'blue']
    class_names = ['Classe A', 'Classe B']
    
    for idx, (model, name) in enumerate(zip(models, model_names)):
        ax = axes[idx]
        
        # Treinar modelo
        model.fit(X_2d, y)
        
        # Fazer prediÃ§Ãµes na grade
        Z = model.predict(np.c_[xx.ravel(), yy.ravel()])
        Z = Z.reshape(xx.shape)
        
        # Plotar regiÃµes de decisÃ£o
        ax.contourf(xx, yy, Z, alpha=0.3, colors=colors)
        
        # Plotar pontos
        for class_id in range(2):
            mask = y == class_id
            class_data = X_2d[mask]
            ax.scatter(class_data[:, 0], class_data[:, 1], 
                      c=colors[class_id], label=class_names[class_id], 
                      alpha=0.7, s=30, edgecolors='black', linewidth=0.5)
        
        # Configurar grÃ¡fico
        ax.set_xlim(x_min, x_max)
        ax.set_ylim(y_min, y_max)
        ax.set_xlabel('PC1')
        ax.set_ylabel('PC2')
        ax.set_title(f'{name}\nAcurÃ¡cia: {model.score(X_2d, y):.3f}')
        ax.legend()
        ax.grid(True, alpha=0.3)
    
    plt.tight_layout()
    
    # Salvar grÃ¡fico
    script_dir = os.path.dirname(os.path.abspath(__file__))
    output_path = os.path.join(script_dir, 'decision_boundaries_2d.png')
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    print(f"ğŸ“Š GrÃ¡fico de fronteiras salvo como 'decision_boundaries_2d.png'")
    
    plt.show()

def save_dataset_5d(X, y):
    """
    Salva o dataset 5D em arquivo CSV.
    """
    
    # Criar DataFrame
    df = pd.DataFrame(X, columns=[f'feature_{i+1}' for i in range(5)])
    df['class'] = y
    df['class_name'] = df['class'].map({0: 'A', 1: 'B'})
    
    # Salvar CSV
    script_dir = os.path.dirname(os.path.abspath(__file__))
    output_path = os.path.join(script_dir, 'dataset_5d.csv')
    df.to_csv(output_path, index=False)
    print(f"ğŸ’¾ Dataset 5D salvo como 'dataset_5d.csv'")
    
    return df


def main():
    """
    FunÃ§Ã£o principal que executa todo o pipeline do ExercÃ­cio 2.
    """
    
    print("ğŸš€ EXERCÃCIO 2: NÃƒO-LINEARIDADE EM DIMENSÃ•ES ALTAS")
    print("="*60)
    
    # Gerar dataset 5D usando NumPy
    print("\nğŸ”µ Gerando dataset 5D com NumPy...")
    X, y, samples_A, samples_B = generate_5d_dataset()
    X_2d, pca_2d, explained_variance = apply_pca_analysis(X, y)
    
    # Visualizar projeÃ§Ã£o 2D
    print("\nğŸ¨ Visualizando projeÃ§Ã£o 2D...")
    visualize_2d_projection(X_2d, y)
    
    # Analisar separabilidade linear
    print("\nğŸ” Analisando separabilidade linear...")
    score_5d, score_2d = analyze_linear_separability(X, y, X_2d)
    
    # Testar redes neurais
    print("\nğŸ§  Testando redes neurais...")
    nn_results = test_neural_networks(X, y)
    
    # Criar grÃ¡ficos de fronteiras de decisÃ£o
    print("\nğŸ¨ Criando fronteiras de decisÃ£o...")
    create_decision_boundary_plot_2d(X_2d, y)
    
    # Salvar dataset
    print("\nğŸ’¾ Salvando dataset...")
    df = save_dataset_5d(X, y)
    
    # Resumo final
    print("\n" + "="*60)
    print("ğŸ“‹ RESUMO DO EXERCÃCIO 2")
    print("="*60)
    
    print("ğŸ¯ Dataset gerado:")
    print("   â€¢ 1000 amostras (500 por classe)")
    print("   â€¢ 5 dimensÃµes por amostra")
    print("   â€¢ 2 classes (A e B)")
    
    print(f"\nğŸ“Š AnÃ¡lise PCA:")
    print(f"   â€¢ VariÃ¢ncia explicada pelos 2 primeiros PCs: {pca_2d.explained_variance_ratio_.sum():.1%}")
    print(f"   â€¢ PC1: {pca_2d.explained_variance_ratio_[0]:.1%}")
    print(f"   â€¢ PC2: {pca_2d.explained_variance_ratio_[1]:.1%}")
    
    print(f"\nğŸ” Separabilidade Linear:")
    print(f"   â€¢ 5D: {score_5d:.3f}")
    print(f"   â€¢ 2D: {score_2d:.3f}")
    
    print(f"\nğŸ§  Melhor Rede Neural:")
    best_nn = max(nn_results, key=lambda x: x[2])
    print(f"   â€¢ {best_nn[0]}: {best_nn[2]:.3f}")
    
    print(f"\nğŸ“ Arquivos gerados:")
    print(f"   â€¢ dataset_5d.csv")
    print(f"   â€¢ dataset_5d_2d_projection.png")
    print(f"   â€¢ decision_boundaries_2d.png")
    
    return X, y, X_2d, df

if __name__ == "__main__":
    X, y, X_2d, df = main()
