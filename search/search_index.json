{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"P\u00e1gina Inicial","text":"Edi\u00e7\u00e3o <p>2025.2</p>"},{"location":"#repositorio","title":"Reposit\u00f3rio","text":""},{"location":"#lincoln-melo","title":"Lincoln Melo","text":""},{"location":"#entregas","title":"Entregas","text":"<ul> <li> Exerc\u00edcio 1 - Data 05/09/2025</li> <li> Exerc\u00edcio 2</li> <li> Exerc\u00edcio 3 </li> <li> Exerc\u00edcio 4 </li> <li> Projeto</li> </ul>"},{"location":"exercise1-data/main/","title":"Exerc\u00edcio 1","text":""},{"location":"exercise1-data/main/#exercicio-1-data","title":"Exerc\u00edcio 1 - Data","text":"<p>Entrega Referente ao exerc\u00edcio 1 do curso de Redes Neurais e Deep Learning</p>"},{"location":"exercise1-data/main/#relatorio-de-entrega","title":"Relat\u00f3rio de entrega!","text":""},{"location":"exercise1-data/main/#questao-1","title":"Quest\u00e3o 1","text":""},{"location":"exercise1-data/main/#11-e-12-geracao-de-dados-e-grafico-das-4-classes","title":"1.1 e 1.2 Gera\u00e7\u00e3o de dados e gr\u00e1fico das 4 classes","text":"<ul> <li>Class 0: Mean = \\([2, 3]\\), Standard Deviation = \\([0.8, 2.5]\\)</li> <li>Class 1: Mean = \\([5, 6]\\), Standard Deviation = \\([1.2, 1.9]\\)</li> <li>Class 2: Mean = \\([8, 1]\\), Standard Deviation = \\([0.9, 0.9]\\)</li> <li>Class 3: Mean = \\([15, 4]\\), Standard Deviation = \\([0.5, 2.0]\\)</li> </ul> <p>Primeiro \u00e9 necess\u00e1rio consegui gerar os dados, ent\u00e3o aqui utilizei a fun\u00e7\u00e3o do numpy para fazer a gera\u00e7\u00e3o dos n\u00fameros com uma seed fixa (n\u00famero escolhido foi o 37). A seguir temos o gr\u00e1fico plotado dos gr\u00e1ficos gerados.</p> <p>teste_simples:</p> 2025-10-26T22:52:23.581084 image/svg+xml Matplotlib v3.10.6, https://matplotlib.org/"},{"location":"exercise1-data/main/#13-analise-e-fronteiras","title":"1.3 An\u00e1lise e fronteiras","text":"<ol> <li> <ol> <li> <ul> <li>Classe 3 apresenta uma maior diferen\u00e7a, o que permite com que separar mais facilmente na regi\u00e3o da direita do gr\u00e1fico.</li> <li>Classe 0, 1 e 2 est\u00e3o bem mais pr\u00f3ximas, o que dificulta uma separa\u00e7\u00e3o entre elas. Vendo o gr\u00e1fico \u00e9 poss\u00edvel perceber que \u00e9 poss\u00edvel tra\u00e7ar uma reta dividindo a classe 2 das classes 0 e 1. Como ela tamb\u00e9m tem um desvio menor a identifica\u00e7\u00e3o dela se torna mais f\u00e1cil.</li> <li>J\u00e1 as entre as classes 0 e 1 existe uma superposi\u00e7\u00e3o maior, por conta da dispers\u00e3o, ent\u00e3o n\u00e3o \u00e9 poss\u00edvel tra\u00e7ar uma linha que dividida perfeitamente elas.</li> </ul> </li> <li> <p>N\u00e3o, \u00e9 imposs\u00edvel separar todas as classes perfeitamente com fronteiras lineares. A classe 3 \u00e9 separ\u00e1vel linearmente das outras, por\u00e9m as classes 0 e 1 tem m\u00falticas sobreposi\u00e7\u00f5es. Ou seja, as fronteiras/bordas precisariam de curvas.</p> </li> <li></li> </ol> </li> </ol> 2025-10-26T22:52:24.092870 image/svg+xml Matplotlib v3.10.6, https://matplotlib.org/"},{"location":"exercise1-data/main/#questao-2","title":"Quest\u00e3o 2","text":""},{"location":"exercise1-data/main/#11","title":"1.1","text":"<p>Realizando a gera\u00e7\u00e3o das amostras e redu\u00e7\u00e3o de dimensionalidade utilizando t\u00e9cnicas de PCA para manter as duas dimens\u00f5es que carregam mais informa\u00e7\u00f5es. Assim, temos o seguinte gr\u00e1fico:</p> <p> 2025-10-26T22:52:24.505821 image/svg+xml Matplotlib v3.10.6, https://matplotlib.org/    Aqui podemos perceber que a Classe A (vermelha): - Se concentra na parte direita do gr\u00e1fico (PC1 positivo) - Centro pr\u00f3ximo da coordenada (2,0) no gr\u00e1fico</p> <p>Sobre a Classe B (azul): - Se concentra na parte esquerda do gr\u00e1fico (PC1 negativo) - Centro pr\u00f3ximo da coordenada (-2,0) no gr\u00e1fico</p> <p>Tem uma regi\u00e3o central na qual existe uma sobreposi\u00e7\u00e3o onde as duas classes se misturam Mesmo com uma tend\u00eancia de separa\u00e7\u00e3o, (um mais negativo e um mais positivo ao longo de PC1),  n\u00e3o existe uma linha reta para poder dividir as duas classes completamente. Ent\u00e3o a linha de fronteira, de divis\u00e3o n\u00e3o \u00e9 trivial, seria necess\u00e1rio uma fun\u00e7\u00e3o mais complexa com curvas. Por isso, seria necess\u00e1rio redes neurais para conseguir tra\u00e7ar essa separa\u00e7\u00e3o complexa de uma forma que realmente conseguisse dividir bem em duas ou  mais regi\u00f5es.</p>"},{"location":"exercise1-data/main/#questao-3","title":"Quest\u00e3o 3","text":""},{"location":"exercise1-data/main/#31-carregamento-e-descricao-dos-dados","title":"3.1 Carregamento e Descri\u00e7\u00e3o dos Dados","text":"<p>O dataset Spaceship Titanic \u00e9 um problema de classifica\u00e7\u00e3o bin\u00e1ria para prever se um passageiro foi transportado para uma dimens\u00e3o alternativa durante um acidente espacial.</p> <p>Features Identificadas:</p> <p>Num\u00e9ricas (6 features): - <code>Age</code>: Idade do passageiro (0-80 anos) - <code>RoomService</code>, - <code>FoodCourt</code> - <code>ShoppingMall</code> - <code>Spa</code> - <code>VRDeck</code>: Gastos em cr\u00e9ditos gal\u00e1cticos</p> <p>Categ\u00f3ricas (7 features): - <code>HomePlanet</code>: Planeta de origem (Earth, Europa, Mars) - <code>CryoSleep</code>: Em sono criog\u00eanico (True/False) - <code>Destination</code>: Destino da viagem (3 planetas) - <code>VIP</code>: Status VIP (True/False) - <code>Cabin</code>: Localiza\u00e7\u00e3o da cabine (formato complexo) - <code>Name</code>, <code>PassengerId</code>: Identificadores</p> <p>An\u00e1lise Detalhada dos valores faltantes:</p> 2025-10-26T22:52:27.355384 image/svg+xml Matplotlib v3.10.6, https://matplotlib.org/"},{"location":"exercise1-data/main/#32-pre-processamento-completo-com-justificativas","title":"3.2 Pr\u00e9-processamento Completo com Justificativas","text":""},{"location":"exercise1-data/main/#estrategia-1-tratamento-de-dados-faltantes","title":"Estrat\u00e9gia 1: Tratamento de Dados Faltantes","text":"<p>Baseado no gr\u00e1fico acima, identificamos dados faltantes em 9 features. Vamos aqui analisar as estrat\u00e9gias para contornar esses valores faltantes.</p> <p>Para Features Num\u00e9ricas:</p> <ul> <li> <p>Age (Idade): ~15.0% faltantes \u2192 Imputa\u00e7\u00e3o pela mediana</p> <ul> <li>Assim conseguimos preservar a distribui\u00e7\u00e3o da idade e dessa forma, tamb\u00e9m evitando um bias no gradiente na etapa de backpropagation</li> </ul> </li> <li> <p>RoomService (Servi\u00e7o Quarto): ~10.4% faltantes \u2192 Preenchimento com zero</p> <ul> <li>Sendo uma aus\u00eancia do registro pode fazer sentido que possa ser igual a 0 cr\u00e9ditos gastos. Al\u00e9m disso, como estamos utilizando a tanh como fun\u00e7\u00e3o de ativa\u00e7\u00e3o, o elemento 0 \u00e9 neutro e n\u00e3o interfere no gradiente.</li> </ul> </li> <li> <p>FoodCourt (Pra\u00e7a Alimenta\u00e7\u00e3o): ~10.2% faltantes \u2192 Preenchimento com zero</p> <ul> <li>Mesma l\u00f3gica do anterior, provavelmente a pessoa n\u00e3o utilizou o servi\u00e7o (0 cr\u00e9ditos) e com o valor 0 n\u00e3o distorce os padr\u00f5es de gasto</li> </ul> </li> <li> <p>ShoppingMall (Shopping): ~10.7% faltantes \u2192 Preenchimento com zero</p> <ul> <li>Mantendo na mesma l\u00f3gica, a pessoa provavelmente n\u00e3o consumiu nada, ent\u00e3o 0. Da mesma forma das duas anteriores.</li> </ul> </li> <li> <p>Spa: ~11.6% faltantes \u2192 Preenchimento com zero</p> <ul> <li>0 = n\u00e3o utilizou servi\u00e7os de spa. 0 N\u00e3o afeta padr\u00f5es e os mantem uniforme</li> </ul> </li> <li> <p>VRDeck (Deck VR): ~11.2% faltantes \u2192 Preenchimento com zero</p> <ul> <li>Novamente 0 por n\u00e3o ter registro pode indicar n\u00e3o ter um uso de fato. E ser neutro para a fun\u00e7\u00e3o de ativa\u00e7\u00e3o.</li> </ul> </li> </ul> <p>Para Features Categ\u00f3ricas:</p> <ul> <li> <p>HomePlanet (Planeta Origem): ~1.1% faltantes \u2192 Imputa\u00e7\u00e3o pela moda</p> <ul> <li>Para preservar distribui\u00e7\u00e3o original e evitar a cria\u00e7\u00e3o de uma outra categoria que s\u00f3 representaria 1%</li> </ul> </li> <li> <p>Cabin (Cabine): ~1.9% faltantes \u2192 Imputa\u00e7\u00e3o pela moda</p> <ul> <li>Novamente, evitar adicionar uma nova categoria e preservando a distribui\u00e7\u00e3o.</li> </ul> </li> <li> <p>Destination (Destino): ~1.1% faltantes \u2192 Imputa\u00e7\u00e3o pela moda</p> <ul> <li>Preserva\u00e7\u00e3o da distribui\u00e7\u00e3o sem adi\u00e7\u00e3o de novas categorias</li> </ul> </li> </ul>"},{"location":"exercise1-data/main/#estrategia-2-encoding-de-variaveis-categoricas","title":"Estrat\u00e9gia 2: Encoding de Vari\u00e1veis Categ\u00f3ricas","text":"<p>One-Hot Encoding para categ\u00f3ricas nominais: - <code>HomePlanet</code> \u2192 3 colunas bin\u00e1rias - <code>Destination</code> \u2192 3 colunas bin\u00e1rias - <code>Cabin</code> (Deck/Side) \u2192 m\u00faltiplas colunas bin\u00e1rias</p> <p>Label Encoding para categ\u00f3ricas booleanas: - <code>CryoSleep</code>: False\u21920, True\u21921 - <code>VIP</code>: False\u21920, True\u21921</p> <p>Justificativa: One-hot evita ordinality artificial, essencial para redes neurais interpretarem categorias independentemente.</p>"},{"location":"exercise1-data/main/#estrategia-3-normalizacao-otimizada-para-funcao-tanh","title":"Estrat\u00e9gia 3: Normaliza\u00e7\u00e3o Otimizada para Fun\u00e7\u00e3o Tanh","text":"<p>M\u00e9todo Escolhido: Z-score normalization</p> <p>Transforma\u00e7\u00e3o Matem\u00e1tica: <pre><code>X_normalized = (X - \u03bc) / \u03c3\n</code></pre></p> <p>Justificativas Te\u00f3ricas:</p> <ol> <li>Regi\u00e3o Ativa da Tanh:</li> <li><code>tanh(x)</code> \u00e9 mais sens\u00edvel em [-2, +2]</li> <li> <p>Utilizando o Z-score centraliza dados em \u03bc=0</p> </li> <li> <p>Converg\u00eancia Otimizada:</p> </li> <li>Gradientes balanceados entre features</li> <li> <p>Evita satura\u00e7\u00e3o da fun\u00e7\u00e3o tanh</p> </li> <li> <p>Preven\u00e7\u00e3o de Problemas:</p> </li> <li>Converg\u00eancia Lenta: Features dominantes mascararam outras</li> </ol>"},{"location":"exercise1-data/main/#33-visualizacoes-demonstrando-o-impacto-tive-problema-com-os-graficos-entao-retirei-essa-parte","title":"3.3 Visualiza\u00e7\u00f5es Demonstrando o Impacto (Tive problema com os gr\u00e1ficos, ent\u00e3o retirei essa parte)","text":""},{"location":"exercise2-perceptron/main/","title":"Perceptron","text":""},{"location":"exercise2-perceptron/main/#parte-1-implementacao-perceptron","title":"Parte 1: Implementa\u00e7\u00e3o Perceptron","text":""},{"location":"exercise2-perceptron/main/#estrutura-do-codigo","title":"Estrutura do C\u00f3digo","text":"<p>Criaremos uma classe <code>Perceptron</code></p> <pre><code>import numpy as np\n\nclass Perceptron:\n    \"\"\"\n    Implementa\u00e7\u00e3o do Perceptron\n\n    Par\u00e2metros\n    ----------\n    learning_rate : float\n        A taxa de aprendizado (entre 0.0 e 1.0)\n    n_iters : int\n        O n\u00famero de passagens (\u00e9pocas)\n\n    Atributos\n    ---------\n    weights\n    bias\n    \"\"\"\n    def __init__(self, learning_rate=0.01, n_iters=10):\n        self.learning_rate = learning_rate\n        self.n_iters = n_iters\n        self.weights = None\n        self.bias = None\n\n    def _activation_function(self, x):\n        \"\"\"\n        Fun\u00e7\u00e3o de Ativa\u00e7\u00e3o Degrau (Heaviside Step Function).\n        Retorna 1 se x &gt;= 0, sen\u00e3o retorna 0.\n        \"\"\"\n        return np.where(x &gt;= 0, 1, 0)\n\n    def fit(self, X, y):\n        \"\"\"\n        Ajusta o modelo aos dados de treinamento.\n\n        Par\u00e2metros\n        ----------\n        X : array-like, shape = [n_samples, n_features]\n            Vetor de treinamento, onde n_samples \u00e9 o n\u00famero de amostras\n            e n_features \u00e9 o n\u00famero de caracter\u00edsticas.\n        y : array-like, shape = [n_samples]\n            Vetor com os r\u00f3tulos (labels) alvo.\n        \"\"\"\n        n_samples, n_features = X.shape\n\n        # Inicializa\u00e7\u00e3o dos pesos e bias\n        self.weights = np.zeros(n_features)\n        self.bias = 0\n\n        y_ = np.array(y)\n\n        for _ in range(self.n_iters):\n            for idx, x_i in enumerate(X):\n                linear_output = np.dot(x_i, self.weights) + self.bias\n                y_predicted = self._activation_function(linear_output)\n                # C\u00e1lculo do Erro e Atualiza\u00e7\u00e3o dos Pesos\n                # A atualiza\u00e7\u00e3o s\u00f3 acontece se houver erro\n                error = y_[idx] - y_predicted\n                update = self.learning_rate * error\n\n                self.weights += update * x_i\n                self.bias += update\n\n    def predict(self, X):\n        linear_output = np.dot(X, self.weights) + self.bias\n        y_predicted = self._activation_function(linear_output)\n        return y_predicted\n</code></pre>"},{"location":"exercise2-perceptron/main/#parte-2-exercicios","title":"Parte 2: Exerc\u00edcios","text":""},{"location":"exercise2-perceptron/main/#exercicio-1-dados-linearmente-separaveis","title":"Exerc\u00edcio 1: Dados Linearmente Separ\u00e1veis","text":""},{"location":"exercise2-perceptron/main/#1-geracao-dos-dados","title":"1. Gera\u00e7\u00e3o dos Dados","text":"<p>Utilizamos o script <code>data_generation.py</code> para criar duas classes de pontos que est\u00e3o distantes uma da outra, garantindo a separabilidade linear. Os par\u00e2metros chave s\u00e3o:</p> <pre><code># data_generation.py\nmean0 = [1.5, 1.5]\ncov0 = [[0.5, 0], [0, 0.5]]\nmean1 = [5, 5]\ncov1 = [[0.5, 0], [0, 0.5]]\n</code></pre> <p>Ao executar o script, obtemos a seguinte visualiza\u00e7\u00e3o:</p> <p> 2025-10-26T22:52:28.144966 image/svg+xml Matplotlib v3.10.6, https://matplotlib.org/ <p> </p> <p>Como podemos ver no gr\u00e1fico, os dois grupos (vermelho e azul) n\u00e3o se misturam, representando um caso ideal para o Perceptron.</p>"},{"location":"exercise2-perceptron/main/#2-treinamento","title":"2. Treinamento","text":"<p>No script <code>test_perceptron.py</code>, instanciamos nosso Perceptron e o treinamos com 80% dos dados gerados, utilizando o m\u00e9todo <code>fit</code>.</p> <pre><code># Trecho de test_perceptron.py\n# ...\np = Perceptron(learning_rate=0.01, n_iters=10)\np.fit(X_train, y_train)\n</code></pre>"},{"location":"exercise2-perceptron/main/#3-avaliacao-evaluation","title":"3. Avalia\u00e7\u00e3o (Evaluation)","text":"<p>Ap\u00f3s o treinamento, usamos os 20% de dados restantes (o conjunto de teste) para verificar o qu\u00e3o bem o modelo generaliza para dados n\u00e3o vistos.</p> <pre><code># Trecho de test_perceptron.py\npredictions = p.predict(X_test)\nprint(f\"Acur\u00e1cia do Perceptron no conjunto de teste: {accuracy(y_test, predictions):.2f}\")\n</code></pre> <p>Resultado Obtido: Ao executar o script, a acur\u00e1cia impressa no terminal \u00e9 1.00 (ou 100%).</p>"},{"location":"exercise2-perceptron/main/#4-analise","title":"4. An\u00e1lise","text":"<p>O resultado de 100% de acur\u00e1cia confirma o Teorema da Converg\u00eancia do Perceptron, que afirma que o algoritmo sempre encontrar\u00e1 uma solu\u00e7\u00e3o (uma linha de separa\u00e7\u00e3o) em um n\u00famero finito de passos, desde que os dados sejam linearmente separ\u00e1veis. </p> <p>Podemos visualizar essa solu\u00e7\u00e3o plotando a fronteira de decis\u00e3o que o Perceptron aprendeu:</p> <p>Acur\u00e1cia do Perceptron no conjunto de teste: 1.00</p> <p> 2025-10-26T22:52:29.252739 image/svg+xml Matplotlib v3.10.6, https://matplotlib.org/ <p> </p> <p>A linha preta representa a fronteira final encontrada pelo algoritmo. Qualquer ponto de um lado da linha \u00e9 classificado como uma classe, e qualquer ponto do outro lado \u00e9 classificado como a outra, separando perfeitamente os dados, que s\u00e3o claramente linearmente separ\u00e1veis.</p>"},{"location":"exercise2-perceptron/main/#exercicio-2-dados-com-sobreposicao-nao-linearmente-separaveis","title":"Exerc\u00edcio 2: Dados com Sobreposi\u00e7\u00e3o (N\u00e3o Linearmente Separ\u00e1veis)","text":""},{"location":"exercise2-perceptron/main/#1-geracao-dos-dados_1","title":"1. Gera\u00e7\u00e3o dos Dados","text":"<p>Utilizamos o script <code>data_generation2.py</code> para criar duas classes de pontos com m\u00e9dias mais pr\u00f3ximas e maior vari\u00e2ncia, o que causa uma sobreposi\u00e7\u00e3o entre elas. Os par\u00e2metros chave s\u00e3o:</p> <pre><code># data_generation2.py\nmean0 = [3, 3]\ncov0 = [[1.5, 0], [0, 1.5]]\nmean1 = [4, 4]\ncov1 = [[1.5, 0], [0, 1.5]]\n</code></pre> <p>Ao executar o script, obtemos a seguinte visualiza\u00e7\u00e3o, onde a mistura entre os pontos vermelhos e azuis \u00e9 clara:</p> <p> 2025-10-26T22:52:31.124734 image/svg+xml Matplotlib v3.10.6, https://matplotlib.org/ <p> </p>"},{"location":"exercise2-perceptron/main/#2-treinamento_1","title":"2. Treinamento","text":"<p>O treinamento, realizado pelo script <code>test_perceptron2.py</code>, segue o mesmo procedimento. O Perceptron tentar\u00e1 encontrar a melhor linha reta poss\u00edvel para separar as duas classes.</p> <pre><code># Trecho de test_perceptron2.py\np = Perceptron(learning_rate=0.01, n_iters=10)\np.fit(X_train, y_train)\n</code></pre>"},{"location":"exercise2-perceptron/main/#3-avaliacao-evaluation_1","title":"3. Avalia\u00e7\u00e3o (Evaluation)","text":"<p>A avalia\u00e7\u00e3o \u00e9 feita da mesma forma, usando o conjunto de teste. No entanto, o resultado esperado \u00e9 diferente.</p> <pre><code># test_perceptron2.py\npredictions = p.predict(X_test)\nprint(f\"Acur\u00e1cia do Perceptron no conjunto de teste com sobreposi\u00e7\u00e3o: {accuracy(y_test, predictions):.2f}\")\n</code></pre> <p>Resultado Obtido: Ao executar o script, a acur\u00e1cia impressa fica em torno de 0.66 (ou 66%). Este valor pode variar ligeiramente devido \u00e0 aleatoriedade na divis\u00e3o treino-teste, mas nunca chegar\u00e1 a 100%.</p>"},{"location":"exercise2-perceptron/main/#4-analise_1","title":"4. An\u00e1lise","text":"<p>A acur\u00e1cia abaixo de 100% demonstra a principal limita\u00e7\u00e3o do Perceptron: ele s\u00f3 pode encontrar solu\u00e7\u00f5es perfeitas para problemas linearmente separ\u00e1veis. Como os dados se sobrep\u00f5em, n\u00e3o existe uma \u00fanica linha reta que consiga dividir as duas classes sem cometer erros.</p> <p>O algoritmo n\u00e3o \"converge\" para uma solu\u00e7\u00e3o sem erros. Em vez disso, a fronteira de decis\u00e3o pode oscilar um pouco durante o treinamento. O resultado final, ap\u00f3s o n\u00famero definido de itera\u00e7\u00f5es, \u00e9 a melhor tentativa do Perceptron de minimizar os erros com uma \u00fanica linha.</p> <p>A fronteira de decis\u00e3o, visualizada ao executar o teste, mostra essa tentativa:</p> <p>Acur\u00e1cia do Perceptron no conjunto de teste com sobreposi\u00e7\u00e3o: 0.66</p> <p> 2025-10-26T22:52:34.112260 image/svg+xml Matplotlib v3.10.6, https://matplotlib.org/ <p> </p> <p>A linha preta corta atrav\u00e9s da \u00e1rea de sobreposi\u00e7\u00e3o, classificando incorretamente alguns pontos de ambas as classes que est\u00e3o do lado \"errado\" da fronteira. Este \u00e9 o comportamento esperado e ilustra por que modelos mais complexos, como Redes Neurais de M\u00faltiplas Camadas (MLPs), s\u00e3o necess\u00e1rios para resolver problemas n\u00e3o-lineares.</p>"},{"location":"exercise3-MLP/main/","title":"Exerc\u00edcio 3","text":""},{"location":"exercise3-MLP/main/#exercicio-1-desenvolvimento-mlp","title":"Exerc\u00edcio 1 - Desenvolvimento MLP","text":""},{"location":"exercise3-MLP/main/#desenvolvimento-parte-1-exercicio-1","title":"Desenvolvimento parte 1 (Exerc\u00edcio 1)","text":""},{"location":"exercise3-MLP/main/#desenvolvimento-parte-2-exercicio-1","title":"Desenvolvimento parte 2 (Exerc\u00edcio 1)","text":""},{"location":"exercise3-MLP/main/#exercicio-2-classificacao-binaria","title":"Exerc\u00edcio 2 \u2014 Classifica\u00e7\u00e3o Bin\u00e1ria","text":"<ul> <li>Dados: 1000 amostras, com 1 cluster para a classe 0 e 2 clusters para a classe 1, garantindo um cen\u00e1rio ligeiramente desafiador e separ\u00e1vel.</li> </ul>"},{"location":"exercise3-MLP/main/#geracao-de-dados-make_classification","title":"Gera\u00e7\u00e3o de Dados (make_classification)","text":"<p>Para realizar a gera\u00e7\u00e3o de diferente de clusters por classe (1 para a classe 0 e 2 para a classe 1), a fun\u00e7\u00e3o <code>make_classification</code> foi utilizada duas vezes: - 1: <code>weights=[1.0, 0.0]</code>, <code>n_clusters_per_class=1</code> \u2192 gera classe 0, com 1 cluster. - 2: <code>weights=[0.0, 1.0]</code>, <code>n_clusters_per_class=2</code> \u2192 gera classe 1, com 2 clusters.</p> <p>Par\u00e2metros principais utilizados: - <code>n_features=2</code>, <code>n_informative=2</code>, <code>n_redundant=0</code>. - <code>class_sep\u22481.5\u20131.6</code> e <code>flip_y\u22480.01</code>, - <code>random_state/seed=42</code> </p> <p>Ap\u00f3s gerar as amostras, os dados s\u00e3o embaralhados, divididos em treino (80%) e teste (20%), e padronizados (m\u00e9dia 0, desvio 1) usando apenas as estat\u00edsticas do conjunto de treino.</p>"},{"location":"exercise3-MLP/main/#mlp","title":"MLP","text":""},{"location":"exercise3-MLP/main/#arquitetura","title":"Arquitetura","text":"<ul> <li>Entrada: 2 features.</li> <li>Camadas ocultas: por padr\u00e3o, <code>[16, 16]</code> neur\u00f4nios com ReLU.</li> <li>Sa\u00edda: 1 neur\u00f4nio com sigmoid para probabilidade da classe 1.</li> </ul>"},{"location":"exercise3-MLP/main/#inicializacao-de-pesos","title":"Inicializa\u00e7\u00e3o de Pesos","text":"<ul> <li>Os pesos s\u00e3o inicializados com valores aleat\u00f3rios pequenos de uma distribui\u00e7\u00e3o normal para quebrar a simetria.</li> </ul>"},{"location":"exercise3-MLP/main/#funcoes-de-ativacao-justificativa","title":"Fun\u00e7\u00f5es de Ativa\u00e7\u00e3o \u2014 justificativa","text":"<ul> <li>ReLU nas ocultas: simples, eficiente e reduz problemas de gradientes muito pequenos em compara\u00e7\u00e3o ao sigmoid/tanh.</li> <li>Sigmoid na sa\u00edda: adequada para classifica\u00e7\u00e3o bin\u00e1ria, pois modela P(y=1|x) \u2208 (0,1).</li> </ul>"},{"location":"exercise3-MLP/main/#funcao-de-perda-justificativa","title":"Fun\u00e7\u00e3o de Perda \u2014 justificativa","text":"<ul> <li>Binary Cross-Entropy (BCE): \u00e9 a perda can\u00f4nica para classifica\u00e7\u00e3o bin\u00e1ria probabil\u00edstica. \u00c9 consistente com a sa\u00edda sigmoid e fornece gradientes bem comportados.</li> </ul>"},{"location":"exercise3-MLP/main/#otimizador-justificativa","title":"Otimizador \u2014 justificativa","text":"<ul> <li>Descida do Gradiente (taxa de aprendizado padr\u00e3o <code>0.05</code>): simples, transparente e suficiente para este problema sint\u00e9tico. Taxas diferentes podem ser testadas para balancear estabilidade e velocidade de converg\u00eancia.</li> </ul>"},{"location":"exercise3-MLP/main/#treinamento","title":"Treinamento","text":"<ul> <li>N\u00famero de \u00e9pocas padr\u00e3o: <code>300</code>.</li> <li>Monitoramento: perda de treino por \u00e9poca (BCE). Ao final, s\u00e3o gerados gr\u00e1ficos e m\u00e9tricas.</li> </ul>"},{"location":"exercise3-MLP/main/#avaliacao","title":"Avalia\u00e7\u00e3o","text":"<ul> <li>Acur\u00e1cia no conjunto de teste (20%).</li> <li>Matriz de confus\u00e3o (TP, TN, FP, FN).</li> <li>Fronteira de decis\u00e3o em 2D para visualiza\u00e7\u00e3o.</li> </ul> <p>Gr\u00e1ficos salvos na mesma pasta do script: - <code>training_loss.png</code> \u2014 curva de perda de treinamento. - <code>decision_boundary.png</code> \u2014 fronteira de decis\u00e3o com pontos de treino e teste. - <code>confusion_matrix.png</code> \u2014 matriz de confus\u00e3o.</p>"},{"location":"exercise3-MLP/main/#distribuicao-dos-dados","title":"Distribui\u00e7\u00e3o dos Dados","text":""},{"location":"exercise3-MLP/main/#curva-de-perda-de-treinamento","title":"Curva de Perda de Treinamento","text":""},{"location":"exercise3-MLP/main/#fronteira-de-decisao","title":"Fronteira de Decis\u00e3o","text":""},{"location":"exercise3-MLP/main/#matriz-de-confusao","title":"Matriz de Confus\u00e3o","text":""},{"location":"exercise3-MLP/main/#resultados-esperados-diretriz","title":"Resultados Esperados (diretriz)","text":"<p>Com <code>class_sep</code> por volta de 1.5\u20131.6 e <code>flip_y=0.01</code>, espera-se que a MLP alcance acur\u00e1cia acima de 0.90 no conjunto de teste, mantendo uma fronteira de decis\u00e3o que respeita as 3 regi\u00f5es (1 cluster da classe 0 e 2 da classe 1). Valores exatos podem variar com o seed, taxa de aprendizado e n\u00famero de \u00e9pocas.</p>"},{"location":"exercise3-MLP/main/#exercicio-3-classificacao-multiclasse","title":"Exerc\u00edcio 3 \u2014 Classifica\u00e7\u00e3o Multiclasse","text":""},{"location":"exercise3-MLP/main/#geracao-de-dados-3-classes-clusters-desiguais","title":"Gera\u00e7\u00e3o de Dados (3 classes, clusters desiguais)","text":"<p>Usamos <code>make_classification</code> tr\u00eas vezes, uma para cada classe, for\u00e7ando que cada chamada gere apenas uma das classes (via <code>weights</code>) e definindo um n\u00famero diferente de clusters por classe: - Classe 0: 2 clusters - Classe 1: 3 clusters - Classe 2: 4 clusters</p> <p>Par\u00e2metros: <code>n_features=4</code>, <code>n_informative=4</code>, <code>n_redundant=0</code>, <code>class_sep\u22481.6</code>, <code>flip_y\u22480.01</code>, <code>seed=42</code>. As tr\u00eas partes s\u00e3o concatenadas e embaralhadas. Depois, dividimos em treino (80%) e teste (20%) e padronizamos com estat\u00edsticas do treino.</p>"},{"location":"exercise3-MLP/main/#mlp-multiclasse-reuso-do-exercicio-2","title":"MLP Multiclasse (Reuso do Exerc\u00edcio 2)","text":"<ul> <li>Estrutura id\u00eantica: camadas ocultas com ReLU.</li> <li>Diferen\u00e7as necess\u00e1rias para multiclasse: camada de sa\u00edda com softmax (3 neur\u00f4nios) e categorical cross-entropy como fun\u00e7\u00e3o de perda.</li> <li>Otimizador: Descida do Gradiente com taxa padr\u00e3o <code>0.05</code> (ajust\u00e1vel via CLI).</li> </ul>"},{"location":"exercise3-MLP/main/#treinamento-e-avaliacao","title":"Treinamento e Avalia\u00e7\u00e3o","text":"<ul> <li>\u00c9pocas padr\u00e3o: <code>400</code>.</li> <li>M\u00e9trica: acur\u00e1cia no conjunto de teste.</li> </ul> <p>Arquivos gerados:</p>"},{"location":"exercise3-MLP/main/#curva-de-perda-de-treinamento_1","title":"Curva de Perda de Treinamento","text":""},{"location":"exercise3-MLP/main/#distribuicao-dos-dados-pca-2d","title":"Distribui\u00e7\u00e3o dos Dados (PCA 2D)","text":""},{"location":"exercise3-MLP/main/#matriz-de-confusao_1","title":"Matriz de Confus\u00e3o","text":""},{"location":"exercise3-MLP/main/#resultados-esperados-diretriz_1","title":"Resultados Esperados (diretriz)","text":"<p>Com separa\u00e7\u00e3o moderada (<code>class_sep\u22481.6</code>) e ru\u00eddo leve, espera-se acur\u00e1cia &gt; 0.85 no teste (varia com seed e hiperpar\u00e2metros). A PCA deve mostrar separa\u00e7\u00e3o razo\u00e1vel entre classes, ainda que com sobreposi\u00e7\u00e3o em regi\u00f5es mais dif\u00edceis.</p>"},{"location":"exercise3-MLP/main/#exercicio-4-mlp-mais-profunda-2-camadas-ocultas","title":"Exerc\u00edcio 4 \u2014 MLP Mais Profunda (\u2265 2 camadas ocultas)","text":"<p>Neste passo, repetimos o Exerc\u00edcio 3, por\u00e9m garantindo uma MLP mais profunda, com pelo menos duas camadas ocultas.</p>"},{"location":"exercise3-MLP/main/#arquitetura_1","title":"Arquitetura","text":"<ul> <li>Entrada: 4 features; Sa\u00edda: 3 classes (softmax).</li> <li>Ocultas: pelo menos 2 camadas.</li> <li>Ativa\u00e7\u00f5es: ReLU nas ocultas e softmax na sa\u00edda.</li> <li>Perda: categorical cross-entropy; Otimizador: Gradiente Descendente.</li> </ul>"},{"location":"exercise3-MLP/main/#treinamento-e-avaliacao_1","title":"Treinamento e Avalia\u00e7\u00e3o","text":"<ul> <li>\u00c9pocas sugeridas: <code>450</code> (ajust\u00e1vel).</li> <li>M\u00e9trica: acur\u00e1cia no conjunto de teste.</li> </ul> <p>Arquivos gerados:</p>"},{"location":"exercise3-MLP/main/#curva-de-perda-de-treinamento_2","title":"Curva de Perda de Treinamento","text":""},{"location":"exercise3-MLP/main/#distribuicao-dos-dados-pca-2d_1","title":"Distribui\u00e7\u00e3o dos Dados (PCA 2D)","text":""},{"location":"exercise3-MLP/main/#matriz-de-confusao_2","title":"Matriz de Confus\u00e3o","text":""},{"location":"exercise3-MLP/main/#observacoes","title":"Observa\u00e7\u00f5es","text":"<ul> <li>A profundidade extra tende a capturar melhor a varia\u00e7\u00e3o entre os 2/3/4 clusters por classe, podendo melhorar a acur\u00e1cia. Entretanto, pode demandar mais \u00e9pocas ou tuning fino da taxa de aprendizado.</li> </ul>"},{"location":"exercise4-VAE/","title":"Implementa\u00e7\u00e3o de VAE","text":"<p>Este diret\u00f3rio cont\u00e9m uma implementa\u00e7\u00e3o completa de um Variational Autoencoder (VAE) treinado no conjunto de dados MNIST.</p>"},{"location":"exercise4-VAE/#arquivos","title":"Arquivos","text":"<ul> <li><code>vae.py</code>: Implementa\u00e7\u00e3o principal com modelo VAE, loop de treinamento e visualiza\u00e7\u00f5es</li> <li><code>explain.md</code>: Documenta\u00e7\u00e3o detalhada da arquitetura e abordagem</li> <li><code>README.md</code>: Este arquivo</li> </ul>"},{"location":"exercise4-VAE/#requisitos","title":"Requisitos","text":"<pre><code>torch&gt;=2.0.0\ntorchvision&gt;=0.15.0\nnumpy&gt;=1.24.0\nmatplotlib&gt;=3.7.0\nscikit-learn&gt;=1.2.0\n</code></pre> <p>Instale via: <pre><code>pip install torch torchvision numpy matplotlib scikit-learn\n</code></pre></p>"},{"location":"exercise4-VAE/#uso","title":"Uso","text":"<p>Execute o pipeline completo de treinamento e visualiza\u00e7\u00e3o:</p> <pre><code>python vae.py\n</code></pre> <p>Isto ir\u00e1: 1. Carregar e preparar dados MNIST 2. Treinar o VAE por 20 \u00e9pocas 3. Gerar visualiza\u00e7\u00f5es no diret\u00f3rio <code>./results/</code> 4. Salvar o modelo treinado</p>"},{"location":"exercise4-VAE/#saida","title":"Sa\u00edda","text":"<p>O script gera: - <code>training_loss.png</code>: Curvas de perda de treinamento e valida\u00e7\u00e3o - <code>reconstructions.png</code>: Imagens originais vs reconstru\u00eddas - <code>generated_samples.png</code>: Novas amostras do espa\u00e7o latente - <code>latent_space.png</code>: Visualiza\u00e7\u00e3o 2D do espa\u00e7o latente em t-SNE - <code>vae_model.pth</code>: Pesos do modelo treinado</p>"},{"location":"exercise4-VAE/#resumo-da-arquitetura","title":"Resumo da Arquitetura","text":"<ul> <li>Dimens\u00e3o latente: 20</li> <li>Codificador: 784 \u2192 512 \u2192 256 \u2192 20</li> <li>Decodificador: 20 \u2192 256 \u2192 512 \u2192 784</li> <li>Perda: Reconstru\u00e7\u00e3o (BCE) + Diverg\u00eancia KL</li> </ul>"},{"location":"exercise4-VAE/#caracteristicas-principais","title":"Caracter\u00edsticas Principais","text":"<ul> <li>Truque de reparametriza\u00e7\u00e3o para fluxo de gradiente</li> <li>Perda combinada de reconstru\u00e7\u00e3o e diverg\u00eancia KL</li> <li>Divis\u00e3o treino/valida\u00e7\u00e3o/teste</li> <li>Visualiza\u00e7\u00e3o de espa\u00e7o latente com t-SNE</li> <li>Gera\u00e7\u00e3o de amostras a partir da distribui\u00e7\u00e3o aprendida</li> </ul>"},{"location":"exercise4-VAE/#personalizacao","title":"Personaliza\u00e7\u00e3o","text":"<p>Modifique estes valores em <code>vae.py</code>: - <code>latent_dim</code>: Mude a dimensionalidade do espa\u00e7o latente - <code>epochs</code>: Ajuste a dura\u00e7\u00e3o do treinamento - <code>batch_size</code>: Altere tamanho do lote em <code>load_data()</code> - <code>lr</code>: Ajuste taxa de aprendizado - <code>dataset</code>: Mude entre 'MNIST' e 'FashionMNIST'</p>"},{"location":"exercise4-VAE/#credito-extra","title":"Cr\u00e9dito Extra","text":"<p>O arquivo inclui uma classe <code>StandardAutoencoder</code> para compara\u00e7\u00e3o. Use-a para analisar diferen\u00e7as em qualidade de reconstru\u00e7\u00e3o e gera\u00e7\u00e3o de amostras entre VAE e Autoencoder padr\u00e3o.</p>"},{"location":"exercise4-VAE/ANALYSIS/","title":"Guia de An\u00e1lise - VAE","text":""},{"location":"exercise4-VAE/ANALYSIS/#resumo-da-implementacao","title":"Resumo da Implementa\u00e7\u00e3o","text":"<p>Este diret\u00f3rio cont\u00e9m uma implementa\u00e7\u00e3o completa de Variational Autoencoder treinado no conjunto de dados MNIST.</p>"},{"location":"exercise4-VAE/ANALYSIS/#descricao-dos-arquivos","title":"Descri\u00e7\u00e3o dos Arquivos","text":"<ol> <li>vae.py: Implementa\u00e7\u00e3o principal</li> <li>Arquitetura Codificador/Decodificador</li> <li>VAE com truque de reparametriza\u00e7\u00e3o</li> <li>Fun\u00e7\u00f5es de treinamento e avalia\u00e7\u00e3o</li> <li>Fun\u00e7\u00f5es de visualiza\u00e7\u00e3o</li> <li> <p>Inclui Autoencoder padr\u00e3o para compara\u00e7\u00e3o</p> </li> <li> <p>compare_vae_ae.py: Implementa\u00e7\u00e3o de cr\u00e9dito extra</p> </li> <li>Compara\u00e7\u00e3o lado a lado VAE vs Autoencoder</li> <li>An\u00e1lise de qualidade de reconstru\u00e7\u00e3o</li> <li>Compara\u00e7\u00e3o de visualiza\u00e7\u00e3o do espa\u00e7o latente</li> <li> <p>Compara\u00e7\u00e3o de curvas de perda de treinamento</p> </li> <li> <p>explain.md: Documenta\u00e7\u00e3o detalhada da arquitetura</p> </li> <li>README.md: Instru\u00e7\u00f5es de configura\u00e7\u00e3o e uso</li> </ol>"},{"location":"exercise4-VAE/ANALYSIS/#arquivos-de-saida-gerados","title":"Arquivos de Sa\u00edda Gerados","text":""},{"location":"exercise4-VAE/ANALYSIS/#visualizacoes","title":"Visualiza\u00e7\u00f5es","text":"<p>Executar <code>vae.py</code> gera:</p> <ul> <li>training_loss.png: Mostra converg\u00eancia de perda de treinamento e valida\u00e7\u00e3o</li> <li>Perda do VAE combina reconstru\u00e7\u00e3o e diverg\u00eancia KL</li> <li> <p>Monitore por overfitting (diverg\u00eancia de perda de valida\u00e7\u00e3o)</p> </li> <li> <p>reconstructions.png: Imagens originais vs reconstru\u00eddas</p> </li> <li>Primeira linha: Imagens de entrada</li> <li>Segunda linha: Imagens reconstru\u00eddas</li> <li> <p>Qualidade melhora com \u00e9pocas de treinamento</p> </li> <li> <p>generated_samples.png: Novas imagens amostradas do espa\u00e7o latente</p> </li> <li>Geradas de distribui\u00e7\u00e3o normal aleat\u00f3ria</li> <li>Qualidade depende da organiza\u00e7\u00e3o do espa\u00e7o latente</li> <li> <p>Deve mostrar padr\u00f5es de d\u00edgitos reconhec\u00edveis</p> </li> <li> <p>latent_space.png: Proje\u00e7\u00e3o 2D do t-SNE do espa\u00e7o latente aprendido</p> </li> <li>Cada cor representa uma classe de d\u00edgito</li> <li>VAE bem treinado mostra agrupamento claro de classes</li> <li>Transi\u00e7\u00f5es suaves entre clusters</li> </ul>"},{"location":"exercise4-VAE/ANALYSIS/#arquivos-de-modelo","title":"Arquivos de Modelo","text":"<ul> <li>vae_model.pth: Pesos do modelo treinado</li> <li>Carregue com: <code>model.load_state_dict(torch.load('./results/vae_model.pth'))</code></li> </ul>"},{"location":"exercise4-VAE/ANALYSIS/#pontos-chave-de-analise","title":"Pontos-Chave de An\u00e1lise","text":""},{"location":"exercise4-VAE/ANALYSIS/#1-qualidade-de-reconstrucao","title":"1. Qualidade de Reconstru\u00e7\u00e3o","text":"<p>Examine <code>reconstructions.png</code>: - Detalhes n\u00edtidos vs desfocados - Caracter\u00edsticas de d\u00edgitos preservadas - Artefatos ou distor\u00e7\u00f5es - Clareza de borda e continuidade</p>"},{"location":"exercise4-VAE/ANALYSIS/#2-qualidade-de-geracao","title":"2. Qualidade de Gera\u00e7\u00e3o","text":"<p>Examine <code>generated_samples.png</code>: - Cobertura de modo: Gera todos os tipos de d\u00edgitos? - Diversidade de amostras: Estilos variados dentro das classes? - Desfoque vs nitidez - Reconhecibilidade</p>"},{"location":"exercise4-VAE/ANALYSIS/#3-estrutura-do-espaco-latente","title":"3. Estrutura do Espa\u00e7o Latente","text":"<p>Examine <code>latent_space.png</code>: - Separa\u00e7\u00e3o de classes: Agrupamento claro? - Sobreposi\u00e7\u00e3o de clusters: Similaridade natural? - Transi\u00e7\u00f5es suaves entre classes - Anomalias ou valores discrepantes</p>"},{"location":"exercise4-VAE/ANALYSIS/#4-dinamica-de-treinamento","title":"4. Din\u00e2mica de Treinamento","text":"<p>Examine <code>training_loss.png</code>: - Diminui\u00e7\u00e3o geral de perda - Lacuna treino/valida\u00e7\u00e3o (verifica\u00e7\u00e3o de overfitting) - Velocidade de converg\u00eancia - Magnitude dos valores de perda</p>"},{"location":"exercise4-VAE/ANALYSIS/#metricas-de-desempenho","title":"M\u00e9tricas de Desempenho","text":"<p>Para avaliar sua implementa\u00e7\u00e3o, considere:</p> <ol> <li>Perda de Reconstru\u00e7\u00e3o: Quanto menor, melhor</li> <li>VAE treina mais lentamente devido ao termo KL</li> <li> <p>Pode n\u00e3o atingir menor perda de reconstru\u00e7\u00e3o</p> </li> <li> <p>Diverg\u00eancia KL: Indica estrutura do espa\u00e7o latente</p> </li> <li>KL alto: Espa\u00e7o latente mais estruturado</li> <li> <p>KL baixo: Espa\u00e7o latente colapsado (posteriori para priori)</p> </li> <li> <p>Qualidade de Amostras: Avalia\u00e7\u00e3o subjetiva mas mensur\u00e1vel</p> </li> <li>Amostras geradas devem ser realistas</li> <li> <p>N\u00e3o apenas dados de treinamento memorizados</p> </li> <li> <p>Agrupamento do Espa\u00e7o Latente: Avalia\u00e7\u00e3o visual</p> </li> <li>D\u00edgitos devem agrupar por classe</li> <li>Limites claros entre classes</li> </ol>"},{"location":"exercise4-VAE/ANALYSIS/#analise-de-credito-extra-compare_vae_aepy","title":"An\u00e1lise de Cr\u00e9dito Extra (compare_vae_ae.py)","text":"<p>Executar a compara\u00e7\u00e3o revela:</p> <ul> <li>Reconstru\u00e7\u00e3o VAE vs AE: </li> <li>VAE pode ter reconstru\u00e7\u00f5es ligeiramente mais desfocadas</li> <li> <p>AE pode alcan\u00e7ar reconstru\u00e7\u00e3o perfeita com capacidade suficiente</p> </li> <li> <p>Organiza\u00e7\u00e3o do Espa\u00e7o Latente:</p> </li> <li>VAE: Estruturado com significado sem\u00e2ntico</li> <li> <p>AE: Pode ter clusters desorganizados espalhados</p> </li> <li> <p>Capacidade de Gera\u00e7\u00e3o:</p> </li> <li>VAE: Pode gerar do espa\u00e7o latente</li> <li> <p>AE: N\u00e3o pode gerar amostras significativas</p> </li> <li> <p>Estabilidade de Treinamento:</p> </li> <li>VAE: Perda mais complexa mas melhor generaliza\u00e7\u00e3o</li> <li>AE: Perda mais simples mas propenso a overfitting</li> </ul>"},{"location":"exercise4-VAE/ANALYSIS/#efeitos-de-hiperparametros","title":"Efeitos de Hiperpar\u00e2metros","text":"<p>Modifique e analise:</p> <ol> <li>Dimens\u00f5es Latentes (5, 10, 20, 50):</li> <li>5D: Muito restrito, gera\u00e7\u00e3o ruim</li> <li>20D: Bom equil\u00edbrio</li> <li> <p>50D: Mais capacidade, pode fazer overfitting</p> </li> <li> <p>\u00c9pocas de Treinamento:</p> </li> <li>In\u00edcio: Reconstru\u00e7\u00e3o ruim</li> <li>20: Bom desempenho</li> <li> <p>Mais longo: Poss\u00edvel overfitting</p> </li> <li> <p>Taxa de Aprendizado:</p> </li> <li>Muito alto: Instabilidade de treinamento</li> <li>Muito baixo: Converg\u00eancia lenta</li> <li> <p>1e-3: Bom padr\u00e3o</p> </li> <li> <p>Tamanho do Lote:</p> </li> <li>Afeta ru\u00eddo de gradiente e converg\u00eancia</li> </ol>"},{"location":"exercise4-VAE/ANALYSIS/#executando-em-diferentes-conjuntos-de-dados","title":"Executando em Diferentes Conjuntos de Dados","text":"<p>Para usar Fashion-MNIST em vez de MNIST:</p> <pre><code>train_loader, val_loader, test_loader = load_data(batch_size=64, dataset='FashionMNIST')\n</code></pre> <p>Observa\u00e7\u00f5es: - Padr\u00f5es mais complexos (itens de vestu\u00e1rio) - Pode exigir treinamento mais longo - Clusters do espa\u00e7o latente menos claros inicialmente</p>"},{"location":"exercise4-VAE/ANALYSIS/#resolucao-de-problemas","title":"Resolu\u00e7\u00e3o de Problemas","text":""},{"location":"exercise4-VAE/ANALYSIS/#reconstrucoes-pobres","title":"Reconstru\u00e7\u00f5es Pobres","text":"<ul> <li>Treinar mais tempo (aumentar \u00e9pocas)</li> <li>Aumentar capacidade do modelo (camadas maiores)</li> <li>Ajustar taxa de aprendizado</li> <li>Verificar normaliza\u00e7\u00e3o de dados</li> </ul>"},{"location":"exercise4-VAE/ANALYSIS/#espaco-latente-colapsado","title":"Espa\u00e7o Latente Colapsado","text":"<ul> <li>Aumentar peso de KL (multiplicar perda de KL)</li> <li>Usar abordagem beta-VAE diferente</li> <li>Treinar mais tempo</li> <li>Garantir dimens\u00f5es latentes suficientes</li> </ul>"},{"location":"exercise4-VAE/ANALYSIS/#colapso-de-modo-em-geracao","title":"Colapso de Modo em Gera\u00e7\u00e3o","text":"<ul> <li>Aumentar dimens\u00f5es latentes</li> <li>Regularizar o termo de KL</li> <li>Usar melhor arquitetura codificador/decodificador</li> <li>Treinar em dados mais diversos</li> </ul>"},{"location":"exercise4-VAE/ANALYSIS/#estrutura-do-relatorio","title":"Estrutura do Relat\u00f3rio","text":"<p>Seu relat\u00f3rio de GitHub Pages deve incluir:</p> <ol> <li>Introdu\u00e7\u00e3o ao conceito de VAE</li> <li>Explica\u00e7\u00e3o da arquitetura</li> <li>Visualiza\u00e7\u00f5es com interpreta\u00e7\u00e3o</li> <li>Resultados e descobertas</li> <li>Desafios encontrados</li> <li>Conclus\u00f5es e insights</li> <li>(Opcional) An\u00e1lise de cr\u00e9dito extra</li> </ol>"},{"location":"exercise4-VAE/ANALYSIS/#pontos-chave","title":"Pontos-Chave","text":"<ul> <li>VAE aprende representa\u00e7\u00f5es latentes significativas</li> <li>Equil\u00edbrio entre reconstru\u00e7\u00e3o e regulariza\u00e7\u00e3o</li> <li>Amostras geradas melhoram com treinamento</li> <li>Espa\u00e7o latente se torna estruturado</li> <li>Truque de reparametriza\u00e7\u00e3o permite fluxo de gradiente</li> <li>VAE supera AE em tarefas de gera\u00e7\u00e3o</li> </ul>"},{"location":"exercise4-VAE/ARCHITECTURE_COMPARISON/","title":"Compara\u00e7\u00e3o de Arquitetura VAE","text":""},{"location":"exercise4-VAE/ARCHITECTURE_COMPARISON/#visao-geral","title":"Vis\u00e3o Geral","text":"<p>Este script compara o desempenho de VAEs com diferentes tamanhos de arquitetura: - Pequeno: 20-40 neur\u00f4nios - M\u00e9dio: 100-200 neur\u00f4nios - Grande: 200-400 neur\u00f4nios - Muito Grande: 400-800 neur\u00f4nios</p>"},{"location":"exercise4-VAE/ARCHITECTURE_COMPARISON/#como-executar","title":"Como Executar","text":"<pre><code>python architecture_comparison.py\n</code></pre>"},{"location":"exercise4-VAE/ARCHITECTURE_COMPARISON/#arquivos-gerados","title":"Arquivos Gerados","text":"<p>O script cria 3 arquivos de visualiza\u00e7\u00e3o em <code>./results/</code>:</p>"},{"location":"exercise4-VAE/ARCHITECTURE_COMPARISON/#1-architecture_comparisonpng","title":"1. architecture_comparison.png","text":"<p>Grid 2x2 mostrando para cada arquitetura: - Esquerda: Imagem original vs reconstru\u00edda lado a lado - Direita: Amostra gerada do espa\u00e7o latente</p> <p>Permite visualiza\u00e7\u00e3o r\u00e1pida do efeito da capacidade de rede.</p>"},{"location":"exercise4-VAE/ARCHITECTURE_COMPARISON/#2-architecture_loss_comparisonpng","title":"2. architecture_loss_comparison.png","text":"<p>Gr\u00e1fico de linha comparando a converg\u00eancia de perda: - Eixo X: \u00c9poca de treinamento (0-15) - Eixo Y: Perda normalizada - Cores diferentes para cada configura\u00e7\u00e3o de arquitetura</p> <p>O que observar: - Modelos maiores convergem mais r\u00e1pido - Modelos menores t\u00eam perda final mais alta - Trade-off entre capacidade e velocidade</p>"},{"location":"exercise4-VAE/ARCHITECTURE_COMPARISON/#3-architecture_detailed_comparisonpng","title":"3. architecture_detailed_comparison.png","text":"<p>Grid 4x8 detalhado mostrando: - Linhas: Cada configura\u00e7\u00e3o de arquitetura - Colunas Esquerdas (4): Reconstru\u00e7\u00f5es (original + 3 varia\u00e7\u00f5es) - Colunas Direitas (4): Amostras geradas</p> <p>Permite an\u00e1lise detalhada de qualidade.</p>"},{"location":"exercise4-VAE/ARCHITECTURE_COMPARISON/#analise-esperada","title":"An\u00e1lise Esperada","text":""},{"location":"exercise4-VAE/ARCHITECTURE_COMPARISON/#pequeno-20-40","title":"Pequeno (20-40)","text":"<ul> <li>Capacidade limitada</li> <li>Reconstru\u00e7\u00f5es mais desfocadas</li> <li>Amostras geradas menos realistas</li> <li>Converg\u00eancia mais lenta</li> </ul>"},{"location":"exercise4-VAE/ARCHITECTURE_COMPARISON/#medio-100-200","title":"M\u00e9dio (100-200)","text":"<ul> <li>Boa rela\u00e7\u00e3o capacidade/velocidade</li> <li>Reconstru\u00e7\u00f5es claras</li> <li>Amostras reconhec\u00edveis</li> <li>Equil\u00edbrio entre qualidade e efici\u00eancia</li> </ul>"},{"location":"exercise4-VAE/ARCHITECTURE_COMPARISON/#grande-200-400","title":"Grande (200-400)","text":"<ul> <li>Alta capacidade</li> <li>Reconstru\u00e7\u00f5es muito n\u00edtidas</li> <li>Amostras de boa qualidade</li> <li>Converge rapidamente</li> </ul>"},{"location":"exercise4-VAE/ARCHITECTURE_COMPARISON/#muito-grande-400-800","title":"Muito Grande (400-800)","text":"<ul> <li>M\u00e1xima capacidade</li> <li>Melhor qualidade de reconstru\u00e7\u00e3o</li> <li>Amostras mais refinadas</li> <li>Maior tempo de treinamento</li> <li>Risco de overfitting</li> </ul>"},{"location":"exercise4-VAE/ARCHITECTURE_COMPARISON/#hiperparametros-personalizaveis","title":"Hiperpar\u00e2metros Personaliz\u00e1veis","text":"<p>Edite o script para ajustar:</p> <pre><code>configs = {\n    'Pequeno (20-40)': {'h1': 20, 'h2': 40},\n    'M\u00e9dio (100-200)': {'h1': 100, 'h2': 200},\n    'Grande (200-400)': {'h1': 200, 'h2': 400},\n    'Muito Grande (400-800)': {'h1': 400, 'h2': 800}\n}\n\n# Ou modifique epochs e learning rate\ntrain_vae(model, train_loader, val_loader, epochs=15, lr=1e-3)\n</code></pre>"},{"location":"exercise4-VAE/ARCHITECTURE_COMPARISON/#observacoes-tecnicas","title":"Observa\u00e7\u00f5es T\u00e9cnicas","text":""},{"location":"exercise4-VAE/ARCHITECTURE_COMPARISON/#estrutura-da-arquitetura","title":"Estrutura da Arquitetura","text":"<p>Cada VAE tem a seguinte estrutura parametriz\u00e1vel:</p> <p>Encoder: 784 \u2192 h1 \u2192 h2 \u2192 20 (latent) Decoder: 20 \u2192 h1 \u2192 h2 \u2192 784</p> <p>Onde h1 e h2 variam por configura\u00e7\u00e3o.</p>"},{"location":"exercise4-VAE/ARCHITECTURE_COMPARISON/#numero-de-parametros","title":"N\u00famero de Par\u00e2metros","text":"<ul> <li>Pequeno: ~40K par\u00e2metros</li> <li>M\u00e9dio: ~270K par\u00e2metros</li> <li>Grande: ~500K par\u00e2metros</li> <li>Muito Grande: ~1.2M par\u00e2metros</li> </ul>"},{"location":"exercise4-VAE/ARCHITECTURE_COMPARISON/#tempo-de-treinamento","title":"Tempo de Treinamento","text":"<p>Tempo estimado por configura\u00e7\u00e3o (15 epochs, GPU): - Pequeno: ~30 segundos - M\u00e9dio: ~1 minuto - Grande: ~2 minutos - Muito Grande: ~4-5 minutos</p>"},{"location":"exercise4-VAE/ARCHITECTURE_COMPARISON/#interpretacao-dos-resultados","title":"Interpreta\u00e7\u00e3o dos Resultados","text":""},{"location":"exercise4-VAE/ARCHITECTURE_COMPARISON/#qualidade-de-reconstrucao","title":"Qualidade de Reconstru\u00e7\u00e3o","text":"<p>Quanto maior a arquitetura: - Bordas mais n\u00edtidas - Menos desfoque - Melhor preserva\u00e7\u00e3o de detalhes</p>"},{"location":"exercise4-VAE/ARCHITECTURE_COMPARISON/#qualidade-de-geracao","title":"Qualidade de Gera\u00e7\u00e3o","text":"<p>Quanto maior a arquitetura: - D\u00edgitos mais realistas - Menos artefatos - Maior diversidade</p>"},{"location":"exercise4-VAE/ARCHITECTURE_COMPARISON/#velocidade-de-convergencia","title":"Velocidade de Converg\u00eancia","text":"<p>Redes maiores: - Convergem mais r\u00e1pido inicialmente - Atingem menor perda final - Requerem mais tempo computacional</p>"},{"location":"exercise4-VAE/ARCHITECTURE_COMPARISON/#analise-comparativa","title":"An\u00e1lise Comparativa","text":""},{"location":"exercise4-VAE/ARCHITECTURE_COMPARISON/#quando-usar-cada-tamanho","title":"Quando usar cada tamanho?","text":"<p>Pequeno (20-40): - Demonstra\u00e7\u00e3o educacional - Recursos computacionais limitados - Prototipagem r\u00e1pida</p> <p>M\u00e9dio (100-200): - Bom compromisso - Qualidade razo\u00e1vel - Treinamento r\u00e1pido - Recomendado para maioria dos casos</p> <p>Grande (200-400): - Quando qualidade \u00e9 priorit\u00e1ria - Recursos computacionais dispon\u00edveis - Produ\u00e7\u00e3o</p> <p>Muito Grande (400-800): - M\u00e1xima qualidade necess\u00e1ria - GPU com muita VRAM - Pesquisa/estado da arte</p>"},{"location":"exercise4-VAE/ARCHITECTURE_COMPARISON/#conclusao","title":"Conclus\u00e3o","text":"<p>Este experimento demonstra: 1. Lei dos Rendimentos Decrescentes: Aumento de capacidade n\u00e3o gera aumento proporcional de qualidade 2. Trade-off Qualidade-Velocidade: Modelos maiores s\u00e3o melhores mas mais lentos 3. Arquitetura Ideal: Para MNIST, o tamanho m\u00e9dio geralmente \u00e9 suficiente 4. Import\u00e2ncia do Design: Arquitetura bem escolhida \u00e9 cr\u00edtica para desempenho</p>"},{"location":"exercise4-VAE/IMPLEMENTATION_SUMMARY/","title":"Resumo de Implementa\u00e7\u00e3o - VAE","text":""},{"location":"exercise4-VAE/IMPLEMENTATION_SUMMARY/#inicio-rapido","title":"In\u00edcio R\u00e1pido","text":"<pre><code>python vae.py\n</code></pre> <p>Isto treina um VAE em MNIST e gera 4 arquivos de visualiza\u00e7\u00e3o em <code>./results/</code></p>"},{"location":"exercise4-VAE/IMPLEMENTATION_SUMMARY/#estrutura-do-projeto","title":"Estrutura do Projeto","text":"<pre><code>exercise4-VAE/\n\u251c\u2500\u2500 vae.py                      # Implementa\u00e7\u00e3o principal do VAE\n\u251c\u2500\u2500 compare_vae_ae.py          # Cr\u00e9dito extra: VAE vs Autoencoder\n\u251c\u2500\u2500 latent_dim_analysis.py     # Cr\u00e9dito extra: Efeito da dimens\u00e3o latente\n\u251c\u2500\u2500 explain.md                 # Documenta\u00e7\u00e3o da arquitetura\n\u251c\u2500\u2500 README.md                  # Instru\u00e7\u00f5es de configura\u00e7\u00e3o\n\u251c\u2500\u2500 ANALYSIS.md                # Guia de an\u00e1lise\n\u2514\u2500\u2500 IMPLEMENTATION_SUMMARY.md  # Este arquivo\n</code></pre>"},{"location":"exercise4-VAE/IMPLEMENTATION_SUMMARY/#o-que-foi-implementado","title":"O que foi Implementado","text":""},{"location":"exercise4-VAE/IMPLEMENTATION_SUMMARY/#componentes-principais","title":"Componentes Principais","text":"<ol> <li>Rede Codificadora</li> <li>Entrada: 784 (imagens 28x28 achatadas)</li> <li>Oculta: 512 \u2192 256 neur\u00f4nios</li> <li> <p>Sa\u00edda: mu e logvar (par\u00e2metros da distribui\u00e7\u00e3o latente)</p> </li> <li> <p>Rede Decodificadora</p> </li> <li>Entrada: latent_dim (padr\u00e3o 20)</li> <li>Oculta: 256 \u2192 512 neur\u00f4nios</li> <li> <p>Sa\u00edda: 784 (imagem reconstru\u00edda)</p> </li> <li> <p>Truque de Reparametriza\u00e7\u00e3o</p> </li> <li>Permite fluxo de gradiente atrav\u00e9s de amostragem estoc\u00e1stica</li> <li> <p>F\u00f3rmula: z = mu + std * epsilon</p> </li> <li> <p>Fun\u00e7\u00e3o de Perda</p> </li> <li>Perda de Reconstru\u00e7\u00e3o (BCE)</li> <li>Diverg\u00eancia KL (regulariza\u00e7\u00e3o)</li> <li>Total: Reconstru\u00e7\u00e3o + KL</li> </ol>"},{"location":"exercise4-VAE/IMPLEMENTATION_SUMMARY/#funcionalidades","title":"Funcionalidades","text":"<ul> <li>Carregamento de dados com divis\u00e3o treino/valida\u00e7\u00e3o/teste</li> <li>Loop de treinamento com monitoramento</li> <li>Visualiza\u00e7\u00e3o de reconstru\u00e7\u00e3o</li> <li>Gera\u00e7\u00e3o de amostras do espa\u00e7o latente</li> <li>Proje\u00e7\u00e3o 2D do espa\u00e7o latente (t-SNE)</li> <li>Curvas de perda de treinamento</li> </ul>"},{"location":"exercise4-VAE/IMPLEMENTATION_SUMMARY/#executando-experimentos","title":"Executando Experimentos","text":""},{"location":"exercise4-VAE/IMPLEMENTATION_SUMMARY/#treinamento-basico","title":"Treinamento B\u00e1sico","text":"<pre><code>python vae.py\n</code></pre>"},{"location":"exercise4-VAE/IMPLEMENTATION_SUMMARY/#comparacao-vae-vs-autoencoder","title":"Compara\u00e7\u00e3o VAE vs Autoencoder","text":"<p><pre><code>python compare_vae_ae.py\n</code></pre> Gera compara\u00e7\u00f5es lado a lado.</p>"},{"location":"exercise4-VAE/IMPLEMENTATION_SUMMARY/#analise-de-dimensao-latente-credito-extra","title":"An\u00e1lise de Dimens\u00e3o Latente (Cr\u00e9dito Extra)","text":"<p><pre><code>python latent_dim_analysis.py\n</code></pre> Treina VAEs com latent_dim = [2, 5, 10, 20, 50] Analisa efeito em qualidade de reconstru\u00e7\u00e3o e gera\u00e7\u00e3o.</p>"},{"location":"exercise4-VAE/IMPLEMENTATION_SUMMARY/#arquivos-de-saida","title":"Arquivos de Sa\u00edda","text":"<p>Executar <code>vae.py</code> cria:</p> Arquivo Prop\u00f3sito training_loss.png Converg\u00eancia de perda reconstructions.png Original vs reconstru\u00eddo generated_samples.png Amostras do espa\u00e7o latente latent_space.png Visualiza\u00e7\u00e3o 2D em t-SNE vae_model.pth Pesos treinados"},{"location":"exercise4-VAE/IMPLEMENTATION_SUMMARY/#hiperparametros-principais","title":"Hiperpar\u00e2metros Principais","text":"<p>Podem ser modificados no c\u00f3digo:</p> <ul> <li><code>latent_dim</code>: 20 (padr\u00e3o)</li> <li><code>epochs</code>: 20 (padr\u00e3o)</li> <li><code>batch_size</code>: 64 (padr\u00e3o)</li> <li><code>learning_rate</code>: 1e-3 (padr\u00e3o)</li> <li><code>dataset</code>: 'MNIST' ou 'FashionMNIST'</li> </ul>"},{"location":"exercise4-VAE/IMPLEMENTATION_SUMMARY/#detalhes-da-arquitetura","title":"Detalhes da Arquitetura","text":""},{"location":"exercise4-VAE/IMPLEMENTATION_SUMMARY/#decomposicao-da-perda-do-vae","title":"Decomposi\u00e7\u00e3o da Perda do VAE","text":"<pre><code>Perda Total = Perda de Reconstru\u00e7\u00e3o + Diverg\u00eancia KL\n\nPerda de Reconstru\u00e7\u00e3o = Entropia Cruzada Bin\u00e1ria(x, x_recon)\n                      = \u03a3[x*log(x_recon) + (1-x)*log(1-x_recon)]\n\nDiverg\u00eancia KL = -0.5 * \u03a3[1 + logvar - mu\u00b2 - exp(logvar)]\n               = Mede diferen\u00e7a de N(0,1)\n</code></pre>"},{"location":"exercise4-VAE/IMPLEMENTATION_SUMMARY/#dinamica-de-treinamento","title":"Din\u00e2mica de Treinamento","text":"<ul> <li>\u00c9poca 0-5: Diminui\u00e7\u00e3o r\u00e1pida de perda, reconstru\u00e7\u00e3o melhora</li> <li>\u00c9poca 5-15: Ajuste fino, espa\u00e7o latente se organiza</li> <li>\u00c9poca 15-20: Converg\u00eancia, perda de valida\u00e7\u00e3o plat\u00f4</li> </ul>"},{"location":"exercise4-VAE/IMPLEMENTATION_SUMMARY/#capacidade-do-modelo","title":"Capacidade do Modelo","text":"<ul> <li>Par\u00e2metros totais: ~400K</li> <li>Codificador: ~300K</li> <li>Decodificador: ~300K</li> <li>Compartilhado: M\u00ednimo</li> </ul>"},{"location":"exercise4-VAE/IMPLEMENTATION_SUMMARY/#expectativas-de-desempenho","title":"Expectativas de Desempenho","text":"<ul> <li>Perda de Reconstru\u00e7\u00e3o: ~60-80 (por amostra)</li> <li>Diverg\u00eancia KL: ~2-5 (por amostra)</li> <li>Tempo de Treinamento: ~2-5 min (CPU), ~30 seg (GPU)</li> </ul>"},{"location":"exercise4-VAE/IMPLEMENTATION_SUMMARY/#exemplos-de-personalizacao","title":"Exemplos de Personaliza\u00e7\u00e3o","text":""},{"location":"exercise4-VAE/IMPLEMENTATION_SUMMARY/#mudar-dimensao-latente","title":"Mudar Dimens\u00e3o Latente","text":"<pre><code>model = VAE(latent_dim=10)  # espa\u00e7o latente 10-dimensional\n</code></pre>"},{"location":"exercise4-VAE/IMPLEMENTATION_SUMMARY/#usar-conjunto-de-dados-diferente","title":"Usar Conjunto de Dados Diferente","text":"<pre><code>train_loader, val_loader, test_loader = load_data(dataset='FashionMNIST')\n</code></pre>"},{"location":"exercise4-VAE/IMPLEMENTATION_SUMMARY/#ajustar-treinamento","title":"Ajustar Treinamento","text":"<pre><code>train_losses, val_losses = train_vae(model, train_loader, val_loader, \n                                      epochs=50, lr=1e-4)\n</code></pre>"},{"location":"exercise4-VAE/IMPLEMENTATION_SUMMARY/#entendendo-os-resultados","title":"Entendendo os Resultados","text":""},{"location":"exercise4-VAE/IMPLEMENTATION_SUMMARY/#boa-reconstrucao","title":"Boa Reconstru\u00e7\u00e3o","text":"<ul> <li>Imagens original e reconstru\u00edda s\u00e3o similares</li> <li>D\u00edgitos claramente vis\u00edveis em reconstru\u00e7\u00f5es</li> <li>Sem distor\u00e7\u00f5es maiores</li> </ul>"},{"location":"exercise4-VAE/IMPLEMENTATION_SUMMARY/#boa-geracao","title":"Boa Gera\u00e7\u00e3o","text":"<ul> <li>Amostras geradas mostram padr\u00f5es de d\u00edgitos</li> <li>Cobrem m\u00faltiplos tipos de d\u00edgitos</li> <li>Clareza razo\u00e1vel</li> </ul>"},{"location":"exercise4-VAE/IMPLEMENTATION_SUMMARY/#bom-espaco-latente","title":"Bom Espa\u00e7o Latente","text":"<ul> <li>D\u00edgitos da mesma classe se agrupam</li> <li>Transi\u00e7\u00f5es suaves entre clusters</li> <li>Separa\u00e7\u00e3o clara de classes</li> </ul>"},{"location":"exercise4-VAE/IMPLEMENTATION_SUMMARY/#problemas-comuns-e-solucoes","title":"Problemas Comuns e Solu\u00e7\u00f5es","text":"Problema Solu\u00e7\u00e3o Reconstru\u00e7\u00f5es desfocadas Treinar mais longo, aumentar capacidade Espa\u00e7o latente colapsado Aumentar peso de KL, usar beta-VAE Gera\u00e7\u00e3o ruim Mais dimens\u00f5es latentes, treinamento mais longo Colapso de modo Melhor regulariza\u00e7\u00e3o, variar dim latente"},{"location":"exercise4-VAE/IMPLEMENTATION_SUMMARY/#mapeamento-de-criterios-de-nota","title":"Mapeamento de Crit\u00e9rios de Nota","text":"<p>Sua submiss\u00e3o ser\u00e1 avaliada em:</p> <ol> <li>Corre\u00e7\u00e3o (3 pts)</li> <li>Arquitetura VAE apropriada</li> <li>Reparametriza\u00e7\u00e3o correta</li> <li> <p>C\u00e1lculo correto de perda</p> </li> <li> <p>Treinamento &amp; Avalia\u00e7\u00e3o (1 pt)</p> </li> <li>Loop de treinamento apropriado</li> <li>Monitoramento de perda</li> <li> <p>Avalia\u00e7\u00e3o em valida\u00e7\u00e3o</p> </li> <li> <p>Amostragem (2 pts)</p> </li> <li>Qualidade de amostras geradas</li> <li>Diversidade de amostras</li> <li> <p>Reconhecibilidade</p> </li> <li> <p>Espa\u00e7o Latente (2 pts)</p> </li> <li>Agrupamento claro</li> <li>Organiza\u00e7\u00e3o sem\u00e2ntica</li> <li> <p>Qualidade de visualiza\u00e7\u00e3o</p> </li> <li> <p>Visualiza\u00e7\u00f5es (1 pt)</p> </li> <li>Gr\u00e1ficos claros e leg\u00edveis</li> <li>R\u00f3tulos apropriados</li> <li> <p>Informativos</p> </li> <li> <p>Relat\u00f3rio (1 pt)</p> </li> <li>Explica\u00e7\u00e3o clara</li> <li>Interpreta\u00e7\u00e3o de resultados</li> <li>Insights e conclus\u00f5es</li> </ol>"},{"location":"exercise4-VAE/IMPLEMENTATION_SUMMARY/#opcoes-de-credito-extra","title":"Op\u00e7\u00f5es de Cr\u00e9dito Extra","text":""},{"location":"exercise4-VAE/IMPLEMENTATION_SUMMARY/#1-comparacao-vae-vs-autoencoder","title":"1. Compara\u00e7\u00e3o VAE vs Autoencoder","text":"<p>Execute <code>compare_vae_ae.py</code> e analise: - Diferen\u00e7as de qualidade de reconstru\u00e7\u00e3o - Organiza\u00e7\u00e3o do espa\u00e7o latente - Capacidade de gera\u00e7\u00e3o - Din\u00e2mica de treinamento</p>"},{"location":"exercise4-VAE/IMPLEMENTATION_SUMMARY/#2-analise-de-dimensao-latente","title":"2. An\u00e1lise de Dimens\u00e3o Latente","text":"<p>Execute <code>latent_dim_analysis.py</code> e reporte: - Efeito em qualidade de reconstru\u00e7\u00e3o - Efeito em gera\u00e7\u00e3o de amostras - Mudan\u00e7as em diverg\u00eancia KL - Dimens\u00e3o latente \u00f3tima</p>"},{"location":"exercise4-VAE/IMPLEMENTATION_SUMMARY/#referencias-para-relatorio","title":"Refer\u00eancias para Relat\u00f3rio","text":"<p>Inclua explica\u00e7\u00e3o de: - O que \u00e9 VAE e por que \u00e9 diferente de AE - Prop\u00f3sito do truque de reparametriza\u00e7\u00e3o - Papel de diverg\u00eancia KL em regulariza\u00e7\u00e3o - Interpreta\u00e7\u00e3o de espa\u00e7o latente - Interpreta\u00e7\u00e3o de amostras geradas</p>"},{"location":"exercise4-VAE/IMPLEMENTATION_SUMMARY/#proximas-etapas","title":"Pr\u00f3ximas Etapas","text":"<ol> <li>Executar <code>vae.py</code> para gerar resultados</li> <li>Analisar todas as visualiza\u00e7\u00f5es</li> <li>Escrever relat\u00f3rio em GitHub Pages</li> <li>(Opcional) Executar scripts de compara\u00e7\u00e3o</li> <li>Submeter link do GitHub Pages</li> </ol>"},{"location":"exercise4-VAE/IMPLEMENTATION_SUMMARY/#checklist-de-resolucao-de-problemas","title":"Checklist de Resolu\u00e7\u00e3o de Problemas","text":"<ul> <li>PyTorch instalado corretamente</li> <li>Dados baixados na primeira execu\u00e7\u00e3o</li> <li>Disponibilidade de GPU (opcional)</li> <li>Espa\u00e7o em disco suficiente (~100MB)</li> <li>Python 3.8+</li> </ul>"},{"location":"exercise4-VAE/IMPLEMENTATION_SUMMARY/#questoes-para-considerar-na-analise","title":"Quest\u00f5es para Considerar na An\u00e1lise","text":"<ul> <li>Por que VAE produz reconstru\u00e7\u00f5es mais desfocadas que AE?</li> <li>Como a organiza\u00e7\u00e3o do espa\u00e7o latente emerge?</li> <li>Por que o truque de reparametriza\u00e7\u00e3o \u00e9 necess\u00e1rio?</li> <li>O que controla a diverg\u00eancia KL?</li> <li>Como a dimens\u00e3o latente afeta a qualidade?</li> <li>Podemos interpolar entre amostras?</li> <li>Como a qualidade de gera\u00e7\u00e3o melhora?</li> </ul>"},{"location":"exercise4-VAE/SUBMISSION_GUIDE/","title":"Guia de Submiss\u00e3o - Exerc\u00edcio VAE","text":""},{"location":"exercise4-VAE/SUBMISSION_GUIDE/#visao-geral","title":"Vis\u00e3o Geral","text":"<p>Este guia orienta voc\u00ea atrav\u00e9s de completar e submeter o exerc\u00edcio VAE no GitHub Pages.</p>"},{"location":"exercise4-VAE/SUBMISSION_GUIDE/#passo-1-gerar-resultados","title":"Passo 1: Gerar Resultados","text":"<p>Execute a implementa\u00e7\u00e3o principal para gerar visualiza\u00e7\u00f5es:</p> <pre><code>cd docs/exercise4-VAE\npython vae.py\n</code></pre> <p>Isto cria um diret\u00f3rio <code>results/</code> com: - training_loss.png - reconstructions.png - generated_samples.png - latent_space.png - vae_model.pth</p>"},{"location":"exercise4-VAE/SUBMISSION_GUIDE/#passo-2-opcional-analise-de-credito-extra","title":"Passo 2: (Opcional) An\u00e1lise de Cr\u00e9dito Extra","text":""},{"location":"exercise4-VAE/SUBMISSION_GUIDE/#comparacao-vae-vs-autoencoder","title":"Compara\u00e7\u00e3o VAE vs Autoencoder","text":"<pre><code>python compare_vae_ae.py\n</code></pre> <p>Gera gr\u00e1ficos de compara\u00e7\u00e3o em <code>results/</code>: - comparison_reconstructions.png - comparison_latent_spaces.png - comparison_losses.png</p>"},{"location":"exercise4-VAE/SUBMISSION_GUIDE/#efeito-da-dimensao-latente","title":"Efeito da Dimens\u00e3o Latente","text":"<pre><code>python latent_dim_analysis.py\n</code></pre> <p>Gera an\u00e1lise de dimens\u00f5es latentes: - latent_dim_analysis.png - samples_latent_dim_2.png at\u00e9 samples_latent_dim_50.png</p>"},{"location":"exercise4-VAE/SUBMISSION_GUIDE/#passo-3-preparar-seu-relatorio","title":"Passo 3: Preparar Seu Relat\u00f3rio","text":"<p>Crie um arquivo markdown para GitHub Pages com estas se\u00e7\u00f5es:</p>"},{"location":"exercise4-VAE/SUBMISSION_GUIDE/#1-introducao","title":"1. Introdu\u00e7\u00e3o","text":"<ul> <li>Breve explica\u00e7\u00e3o de VAEs</li> <li>Diferen\u00e7a de Autoencoders padr\u00e3o</li> <li>Por que aprender sobre VAEs</li> </ul>"},{"location":"exercise4-VAE/SUBMISSION_GUIDE/#2-arquitetura","title":"2. Arquitetura","text":"<ul> <li>Estrutura do codificador</li> <li>Estrutura do decodificador</li> <li>Truque de reparametriza\u00e7\u00e3o</li> <li>Componentes da fun\u00e7\u00e3o de perda</li> </ul>"},{"location":"exercise4-VAE/SUBMISSION_GUIDE/#3-detalhes-de-implementacao","title":"3. Detalhes de Implementa\u00e7\u00e3o","text":"<ul> <li>Conjunto de dados usado (MNIST)</li> <li>Configura\u00e7\u00e3o de treinamento</li> <li>Hiperpar\u00e2metros</li> <li>Hardware usado</li> </ul>"},{"location":"exercise4-VAE/SUBMISSION_GUIDE/#4-resultados","title":"4. Resultados","text":"<p>Inclua e explique visualiza\u00e7\u00f5es:</p>"},{"location":"exercise4-VAE/SUBMISSION_GUIDE/#qualidade-de-reconstrucao","title":"Qualidade de Reconstru\u00e7\u00e3o","text":"<ul> <li>Mostre reconstructions.png</li> <li>Analise precis\u00e3o de reconstru\u00e7\u00e3o</li> <li>Discuta artefatos</li> </ul>"},{"location":"exercise4-VAE/SUBMISSION_GUIDE/#geracao-de-amostras","title":"Gera\u00e7\u00e3o de Amostras","text":"<ul> <li>Mostre generated_samples.png</li> <li>Avalie qualidade de amostras</li> <li>Avalie cobertura de modo</li> </ul>"},{"location":"exercise4-VAE/SUBMISSION_GUIDE/#organizacao-do-espaco-latente","title":"Organiza\u00e7\u00e3o do Espa\u00e7o Latente","text":"<ul> <li>Mostre latent_space.png</li> <li>Interprete estrutura de clusters</li> <li>Discuta organiza\u00e7\u00e3o sem\u00e2ntica</li> </ul>"},{"location":"exercise4-VAE/SUBMISSION_GUIDE/#progresso-de-treinamento","title":"Progresso de Treinamento","text":"<ul> <li>Mostre training_loss.png</li> <li>Analise converg\u00eancia</li> <li>Verifique overfitting</li> </ul>"},{"location":"exercise4-VAE/SUBMISSION_GUIDE/#5-descobertas-principais","title":"5. Descobertas Principais","text":"<ul> <li>Desempenho do VAE em MNIST</li> <li>Propriedades do espa\u00e7o latente</li> <li>Qualidade de amostras geradas</li> <li>Trade-off reconstru\u00e7\u00e3o-gera\u00e7\u00e3o</li> </ul>"},{"location":"exercise4-VAE/SUBMISSION_GUIDE/#6-desafios-e-solucoes","title":"6. Desafios e Solu\u00e7\u00f5es","text":"<ul> <li>Desafios encontrados</li> <li>Solu\u00e7\u00f5es implementadas</li> <li>Li\u00e7\u00f5es aprendidas</li> </ul>"},{"location":"exercise4-VAE/SUBMISSION_GUIDE/#7-conclusoes","title":"7. Conclus\u00f5es","text":"<ul> <li>Resumo de descobertas</li> <li>Efetividade do VAE</li> <li>Melhorias futuras</li> </ul>"},{"location":"exercise4-VAE/SUBMISSION_GUIDE/#8-opcional-credito-extra","title":"8. (Opcional) Cr\u00e9dito Extra","text":"<p>Se completado, inclua: - Resultados de compara\u00e7\u00e3o VAE vs AE - Resultados de an\u00e1lise de dimens\u00e3o latente - M\u00e9tricas de desempenho entre configura\u00e7\u00f5es</p>"},{"location":"exercise4-VAE/SUBMISSION_GUIDE/#passo-4-organizar-arquivos","title":"Passo 4: Organizar Arquivos","text":"<p>Organize arquivos em seu reposit\u00f3rio GitHub Pages:</p> <pre><code>repo/\n\u251c\u2500\u2500 index.md (p\u00e1gina principal)\n\u251c\u2500\u2500 exercise4-vae.md (ou similar)\n\u251c\u2500\u2500 results/\n\u2502   \u251c\u2500\u2500 training_loss.png\n\u2502   \u251c\u2500\u2500 reconstructions.png\n\u2502   \u251c\u2500\u2500 generated_samples.png\n\u2502   \u251c\u2500\u2500 latent_space.png\n\u2502   \u2514\u2500\u2500 (arquivos de cr\u00e9dito extra se aplic\u00e1vel)\n\u2514\u2500\u2500 (outros exerc\u00edcios)\n</code></pre>"},{"location":"exercise4-VAE/SUBMISSION_GUIDE/#passo-5-referenciar-seu-codigo","title":"Passo 5: Referenciar Seu C\u00f3digo","text":"<p>Documente sua implementa\u00e7\u00e3o:</p>"},{"location":"exercise4-VAE/SUBMISSION_GUIDE/#secoes-de-codigo-principais-para-explicar","title":"Se\u00e7\u00f5es de C\u00f3digo Principais para Explicar","text":"<ol> <li>Defini\u00e7\u00e3o da Classe VAE</li> <li>Arquitetura Codificador e Decodificador</li> <li> <p>Passagem direta com reparametriza\u00e7\u00e3o</p> </li> <li> <p>Fun\u00e7\u00e3o de Perda</p> </li> <li>C\u00e1lculo de perda de reconstru\u00e7\u00e3o</li> <li>F\u00f3rmula de diverg\u00eancia KL</li> <li> <p>Perda combinada</p> </li> <li> <p>Loop de Treinamento</p> </li> <li>Batching de dados</li> <li>Passo de otimiza\u00e7\u00e3o</li> <li> <p>Monitoramento de perda</p> </li> <li> <p>Fun\u00e7\u00f5es de Visualiza\u00e7\u00e3o</p> </li> <li>Como reconstru\u00e7\u00f5es s\u00e3o geradas</li> <li>Como amostras s\u00e3o criadas</li> <li>Como espa\u00e7o latente \u00e9 visualizado</li> </ol>"},{"location":"exercise4-VAE/SUBMISSION_GUIDE/#passo-6-criar-github-pages","title":"Passo 6: Criar GitHub Pages","text":"<ol> <li>Crie reposit\u00f3rio GitHub para o curso</li> <li>Habilite GitHub Pages em Configura\u00e7\u00f5es</li> <li>Crie arquivos markdown com sua an\u00e1lise</li> <li>Inclua imagens do diret\u00f3rio <code>results/</code></li> <li>Use sintaxe <code>![alt text](path/to/image.png)</code></li> </ol> <p>Exemplo markdown:</p> <pre><code>## Qualidade de Reconstru\u00e7\u00e3o do VAE\n\n![Reconstru\u00e7\u00f5es](results/reconstructions.png)\n\nA qualidade de reconstru\u00e7\u00e3o mostra que o VAE aprendeu com sucesso a \ncodificar e decodificar d\u00edgitos MNIST. As reconstru\u00e7\u00f5es s\u00e3o relativamente \nclaras com leve desfoque t\u00edpico de sa\u00edda VAE.\n</code></pre>"},{"location":"exercise4-VAE/SUBMISSION_GUIDE/#passo-7-submeter","title":"Passo 7: Submeter","text":"<ol> <li>Certifique-se de que seu site GitHub Pages est\u00e1 ao vivo</li> <li>Copie a URL do GitHub Pages</li> <li>Submeta a URL em insper.blackboard.com</li> <li>Formato: <code>https://username.github.io/repo/exercise4-vae</code></li> </ol>"},{"location":"exercise4-VAE/SUBMISSION_GUIDE/#checklist-de-avaliacao","title":"Checklist de Avalia\u00e7\u00e3o","text":"<p>Antes de submeter, verifique:</p>"},{"location":"exercise4-VAE/SUBMISSION_GUIDE/#implementacao-3-pts","title":"Implementa\u00e7\u00e3o (3 pts)","text":"<ul> <li> Arquitetura VAE implementada corretamente</li> <li> Codificador e Decodificador funcionando</li> <li> Truque de reparametriza\u00e7\u00e3o implementado</li> <li> Perda computada corretamente (Reconstru\u00e7\u00e3o + KL)</li> </ul>"},{"location":"exercise4-VAE/SUBMISSION_GUIDE/#treinamento-avaliacao-1-pt","title":"Treinamento &amp; Avalia\u00e7\u00e3o (1 pt)","text":"<ul> <li> Modelo treinado em MNIST</li> <li> Perdas de treinamento e valida\u00e7\u00e3o monitoradas</li> <li> Pelo menos 20 epochs de treinamento</li> </ul>"},{"location":"exercise4-VAE/SUBMISSION_GUIDE/#amostragem-2-pts","title":"Amostragem (2 pts)","text":"<ul> <li> Amostras geradas do espa\u00e7o latente</li> <li> Amostras mostram padr\u00f5es tipo-d\u00edgito</li> <li> M\u00faltiplas amostras diversas geradas</li> </ul>"},{"location":"exercise4-VAE/SUBMISSION_GUIDE/#espaco-latente-2-pts","title":"Espa\u00e7o Latente (2 pts)","text":"<ul> <li> Visualiza\u00e7\u00e3o do espa\u00e7o latente criada</li> <li> Classes se agrupam por tipo de d\u00edgito</li> <li> Transi\u00e7\u00f5es suaves entre clusters</li> </ul>"},{"location":"exercise4-VAE/SUBMISSION_GUIDE/#visualizacoes-1-pt","title":"Visualiza\u00e7\u00f5es (1 pt)","text":"<ul> <li> Todos os gr\u00e1ficos necess\u00e1rios presentes</li> <li> R\u00f3tulos e t\u00edtulos claros</li> <li> Apar\u00eancia profissional</li> </ul>"},{"location":"exercise4-VAE/SUBMISSION_GUIDE/#relatorio-1-pt","title":"Relat\u00f3rio (1 pt)","text":"<ul> <li> Explica\u00e7\u00e3o escrita clara</li> <li> Estrutura organizada</li> <li> Interpreta\u00e7\u00e3o de resultados</li> </ul>"},{"location":"exercise4-VAE/SUBMISSION_GUIDE/#credito-extra-opcional","title":"Cr\u00e9dito Extra Opcional","text":"<ul> <li> Compara\u00e7\u00e3o VAE vs AE (se realizado)</li> <li> An\u00e1lise de dimens\u00e3o latente (se realizado)</li> <li> Experimentos adicionais (se realizado)</li> </ul>"},{"location":"exercise4-VAE/SUBMISSION_GUIDE/#problemas-comuns-na-submissao","title":"Problemas Comuns na Submiss\u00e3o","text":""},{"location":"exercise4-VAE/SUBMISSION_GUIDE/#problema-imagens-nao-carregam-no-github-pages","title":"Problema: Imagens n\u00e3o carregam no GitHub Pages","text":"<ul> <li>Certifique-se de que caminhos s\u00e3o relativos</li> <li>Verifique que imagens est\u00e3o no reposit\u00f3rio</li> <li>Verifique que <code>.gitignore</code> n\u00e3o exclui imagens</li> </ul>"},{"location":"exercise4-VAE/SUBMISSION_GUIDE/#problema-problemas-de-formatacao-markdown","title":"Problema: Problemas de formata\u00e7\u00e3o Markdown","text":"<ul> <li>Valide sintaxe markdown</li> <li>Verifique sintaxe de imagem: <code>![alt](path/image.png)</code></li> <li>Use hierarquia apropriada de cabe\u00e7alhos</li> </ul>"},{"location":"exercise4-VAE/SUBMISSION_GUIDE/#problema-referencias-de-codigo-pouco-claras","title":"Problema: Refer\u00eancias de c\u00f3digo pouco claras","text":"<ul> <li>Inclua snippets de c\u00f3digo no relat\u00f3rio</li> <li>Explique detalhes principais de implementa\u00e7\u00e3o</li> <li>Vincule ao reposit\u00f3rio GitHub se necess\u00e1rio</li> </ul>"},{"location":"exercise4-VAE/SUBMISSION_GUIDE/#template-de-relatorio","title":"Template de Relat\u00f3rio","text":"<p>Use esta estrutura para seu relat\u00f3rio GitHub Pages:</p> <pre><code># Variational Autoencoder (VAE) - Exerc\u00edcio 4\n\n## Introdu\u00e7\u00e3o\n\n## Arquitetura\n\n## Detalhes de Implementa\u00e7\u00e3o\n\n## Resultados\n\n### Reconstru\u00e7\u00f5es\n\n### Gera\u00e7\u00e3o de Amostras\n\n### Espa\u00e7o Latente\n\n### Progresso de Treinamento\n\n## An\u00e1lise\n\n## Desafios e Solu\u00e7\u00f5es\n\n## Conclus\u00f5es\n\n## Refer\u00eancias\n\n(Opcional) An\u00e1lise de Cr\u00e9dito Extra\n</code></pre>"},{"location":"exercise4-VAE/SUBMISSION_GUIDE/#lembrete-de-prazo","title":"Lembrete de Prazo","text":"<ul> <li>Prazo: Domingo, 26 de outubro de 2025</li> <li>Hora: 23:59</li> <li>Formato: Apenas link de GitHub Pages</li> <li>Submiss\u00e3o: insper.blackboard.com</li> </ul>"},{"location":"exercise4-VAE/SUBMISSION_GUIDE/#recursos-de-suporte","title":"Recursos de Suporte","text":"<ul> <li>Arquivos de implementa\u00e7\u00e3o em <code>docs/exercise4-VAE/</code></li> <li><code>explain.md</code>: Detalhes de arquitetura</li> <li><code>README.md</code>: Instru\u00e7\u00f5es de configura\u00e7\u00e3o</li> <li><code>ANALYSIS.md</code>: Guia de an\u00e1lise detalhado</li> <li><code>IMPLEMENTATION_SUMMARY.md</code>: Refer\u00eancia r\u00e1pida</li> </ul>"},{"location":"exercise4-VAE/SUBMISSION_GUIDE/#questoes-a-responder-no-relatorio","title":"Quest\u00f5es a Responder no Relat\u00f3rio","text":"<p>Seu relat\u00f3rio deve responder:</p> <ol> <li>O que \u00e9 um VAE e como difere de um autoencoder regular?</li> <li>O que \u00e9 o truque de reparametriza\u00e7\u00e3o e por que \u00e9 necess\u00e1rio?</li> <li>O que o termo diverg\u00eancia KL na fun\u00e7\u00e3o de perda faz?</li> <li>Com que precis\u00e3o seu VAE reconstr\u00f3i d\u00edgitos MNIST?</li> <li>Com que qualidade seu VAE gera novas amostras?</li> <li>Que estrutura emerge no espa\u00e7o latente?</li> <li>Que desafios voc\u00ea enfrentou e como os resolveu?</li> <li>O que voc\u00ea melhoraria em sua implementa\u00e7\u00e3o?</li> </ol>"},{"location":"exercise4-VAE/SUBMISSION_GUIDE/#checklist-final","title":"Checklist Final","text":"<ul> <li> C\u00f3digo executa sem erros</li> <li> Todas as visualiza\u00e7\u00f5es geradas</li> <li> Site GitHub Pages criado</li> <li> Relat\u00f3rio escrito e formatado</li> <li> Imagens incorporadas no relat\u00f3rio</li> <li> URL funciona e exibe corretamente</li> <li> C\u00f3digo compreens\u00edvel e documentado</li> <li> Relat\u00f3rio explica todas as descobertas</li> <li> Opcional: Cr\u00e9dito extra completado</li> <li> URL submetido ao Blackboard</li> </ul>"},{"location":"exercise4-VAE/explain/","title":"Relat\u00f3rio de Implementa\u00e7\u00e3o - VAE","text":""},{"location":"exercise4-VAE/explain/#visao-geral","title":"Vis\u00e3o Geral","text":"<p>Este documento explica a implementa\u00e7\u00e3o do Variational Autoencoder (VAE) para o conjunto de dados MNIST. O VAE \u00e9 um modelo generativo que aprende a codificar imagens em um espa\u00e7o latente e decodific\u00e1-las novamente para reconstruir os originais.</p>"},{"location":"exercise4-VAE/explain/#arquitetura","title":"Arquitetura","text":""},{"location":"exercise4-VAE/explain/#rede-codificadora","title":"Rede Codificadora","text":"<ul> <li>Camada de entrada: 784 dimens\u00f5es (imagens 28x28 achatadas)</li> <li>Camada oculta 1: 512 neur\u00f4nios com ativa\u00e7\u00e3o ReLU</li> <li>Camada oculta 2: 256 neur\u00f4nios com ativa\u00e7\u00e3o ReLU</li> <li>Sa\u00edda: Dois ramos produzindo mu (m\u00e9dia) e logvar (log-vari\u00e2ncia) de forma latent_dim</li> </ul> <p>O codificador mapeia imagens de entrada para uma distribui\u00e7\u00e3o de probabilidade no espa\u00e7o latente.</p>"},{"location":"exercise4-VAE/explain/#rede-decodificadora","title":"Rede Decodificadora","text":"<ul> <li>Camada de entrada: latent_dim dimens\u00f5es (vetor latente amostrado)</li> <li>Camada oculta 1: 256 neur\u00f4nios com ativa\u00e7\u00e3o ReLU</li> <li>Camada oculta 2: 512 neur\u00f4nios com ativa\u00e7\u00e3o ReLU</li> <li>Camada de sa\u00edda: 784 dimens\u00f5es com ativa\u00e7\u00e3o Sigmoid</li> </ul> <p>O decodificador reconstr\u00f3i imagens a partir dos vetores latentes.</p>"},{"location":"exercise4-VAE/explain/#truque-de-reparametrizacao","title":"Truque de Reparametriza\u00e7\u00e3o","text":"<p>O truque de reparametriza\u00e7\u00e3o permite retropropaga\u00e7\u00e3o atrav\u00e9s da opera\u00e7\u00e3o de amostragem: - Amostra epsilon de N(0, 1) - Calcula z = mu + std * epsilon, onde std = exp(0.5 * logvar) - Isso permite que gradientes fluam atrav\u00e9s do processo de amostragem</p>"},{"location":"exercise4-VAE/explain/#funcao-de-perda","title":"Fun\u00e7\u00e3o de Perda","text":"<p>A perda do VAE combina dois componentes:</p> <ol> <li>Perda de Reconstru\u00e7\u00e3o: Perda de Entropia Cruzada Bin\u00e1ria (BCE) entre imagens originais e reconstru\u00eddas</li> <li>Diverg\u00eancia KL: Diverg\u00eancia de Kullback-Leibler entre a distribui\u00e7\u00e3o aprendida e normal padr\u00e3o N(0, 1)</li> </ol> <p>Perda Total = Perda de Reconstru\u00e7\u00e3o + Diverg\u00eancia KL</p> <p>Este objetivo incentiva o modelo a reconstruir entradas com precis\u00e3o e manter um espa\u00e7o latente estruturado.</p>"},{"location":"exercise4-VAE/explain/#preparacao-de-dados","title":"Prepara\u00e7\u00e3o de Dados","text":"<ul> <li>Conjunto de dados: MNIST (d\u00edgitos manuscritos)</li> <li>Normaliza\u00e7\u00e3o: Imagens convertidas para intervalo [0, 1] usando ToTensor</li> <li>Divis\u00e3o Treino/Valida\u00e7\u00e3o: 80/20 dos dados de treinamento</li> <li>Conjunto de teste: Conjunto de teste oficial do MNIST</li> <li>Tamanho do lote: 64</li> </ul>"},{"location":"exercise4-VAE/explain/#detalhes-do-treinamento","title":"Detalhes do Treinamento","text":"<ul> <li>Otimizador: Adam com taxa de aprendizado 1e-3</li> <li>\u00c9pocas: 20</li> <li>O treinamento monitora perda de treinamento e valida\u00e7\u00e3o</li> <li>Parada antecipada n\u00e3o implementada mas pode ser adicionada</li> <li>Dispositivo: GPU se dispon\u00edvel, caso contr\u00e1rio CPU</li> </ul>"},{"location":"exercise4-VAE/explain/#componentes-principais","title":"Componentes Principais","text":""},{"location":"exercise4-VAE/explain/#carregamento-de-dados","title":"Carregamento de Dados","text":"<p>A fun\u00e7\u00e3o <code>load_data()</code> cuida de baixar MNIST/Fashion-MNIST e criar dataloaders com divis\u00f5es apropriadas treino/valida\u00e7\u00e3o/teste.</p>"},{"location":"exercise4-VAE/explain/#loop-de-treinamento","title":"Loop de Treinamento","text":"<p>A fun\u00e7\u00e3o <code>train_vae()</code> implementa o procedimento completo de treinamento: - Passagem direta atrav\u00e9s do codificador e decodificador - C\u00e1lculo da perda - Passagem inversa e otimiza\u00e7\u00e3o - Monitoramento de valida\u00e7\u00e3o</p>"},{"location":"exercise4-VAE/explain/#visualizacoes","title":"Visualiza\u00e7\u00f5es","text":"<ol> <li>Reconstru\u00e7\u00f5es: Compara imagens originais e reconstru\u00eddas lado a lado</li> <li>Amostras Geradas: Cria novas imagens amostrando do espa\u00e7o latente</li> <li>Espa\u00e7o Latente: Visualiza\u00e7\u00e3o 2D usando t-SNE para dimens\u00f5es &gt; 2</li> <li>Perda de Treinamento: Plota perda de treinamento e valida\u00e7\u00e3o sobre \u00e9pocas</li> </ol>"},{"location":"exercise4-VAE/explain/#visualizacao-do-espaco-latente","title":"Visualiza\u00e7\u00e3o do Espa\u00e7o Latente","text":"<p>Quando dimens\u00e3o latente &gt; 2, t-SNE \u00e9 usado para projetar para 2D para visualiza\u00e7\u00e3o. O gr\u00e1fico resultante mostra como diferentes classes de d\u00edgitos se agrupam no espa\u00e7o latente aprendido.</p>"},{"location":"exercise4-VAE/explain/#amostras-geradas","title":"Amostras Geradas","text":"<p>Novas amostras s\u00e3o geradas por: 1. Amostragem de vetores aleat\u00f3rios de distribui\u00e7\u00e3o normal padr\u00e3o 2. Passagem atrav\u00e9s do decodificador 3. Visualiza\u00e7\u00e3o em formato de grade</p> <p>A qualidade melhora com treinamento mais longo conforme o decodificador aprende melhores representa\u00e7\u00f5es.</p>"},{"location":"exercise4-VAE/explain/#arquivos-gerados","title":"Arquivos Gerados","text":"<ul> <li><code>training_loss.png</code>: Curvas de perda sobre \u00e9pocas</li> <li><code>reconstructions.png</code>: Imagens originais vs reconstru\u00eddas</li> <li><code>generated_samples.png</code>: Novas amostras do espa\u00e7o latente</li> <li><code>latent_space.png</code>: Visualiza\u00e7\u00e3o 2D do espa\u00e7o latente</li> <li><code>vae_model.pth</code>: Pesos do modelo treinado</li> </ul>"},{"location":"exercise4-VAE/explain/#como-executar","title":"Como Executar","text":"<pre><code>python vae.py\n</code></pre> <p>Certifique-se de que PyTorch, torchvision, scikit-learn, matplotlib e numpy est\u00e3o instalados.</p>"},{"location":"exercise4-VAE/explain/#principais-insights","title":"Principais Insights","text":"<ol> <li>VAE aprende a comprimir imagens em representa\u00e7\u00f5es latentes significativas</li> <li>Qualidade de reconstru\u00e7\u00e3o melhora com treinamento</li> <li>Espa\u00e7o latente se torna organizado com estrutura sem\u00e2ntica</li> <li>Amostras geradas melhoram gradualmente conforme o treinamento progride</li> <li>Maiores dimens\u00f5es latentes capturam mais detalhes mas podem causar overfitting</li> </ol>"},{"location":"exercise4-VAE/explain/#experimentos-adicionais","title":"Experimentos Adicionais","text":"<ul> <li>Comparar com Autoencoder padr\u00e3o (AE)</li> <li>Variar dimens\u00f5es latentes (5, 10, 20, 50) para ver efeitos</li> <li>Treinar em Fashion-MNIST para distribui\u00e7\u00e3o de dados diferente</li> <li>Experimentar com diferentes arquiteturas de rede</li> <li>Analisar interpola\u00e7\u00e3o entre pontos no espa\u00e7o latente</li> </ul>"},{"location":"exercise4-VAE/main/","title":"Exerc\u00edcio 4 - Variational Autoencoder (VAE)","text":""},{"location":"exercise4-VAE/main/#o-que-foi-feito","title":"O que foi feito","text":"<p>Implementei um Variational Autoencoder (VAE) do zero usando PyTorch no dataset MNIST. O objetivo era entender como funcionam esses modelos generativos, que conseguem n\u00e3o s\u00f3 reconstruir imagens como tamb\u00e9m gerar novas amostras sinteticamente.</p>"},{"location":"exercise4-VAE/main/#como-funciona-a-arquitetura","title":"Como funciona a arquitetura","text":""},{"location":"exercise4-VAE/main/#o-codificador","title":"O Codificador","text":"<p>O codificador pega uma imagem e a comprime em um vetor latente com distribui\u00e7\u00e3o gaussiana. A ideia \u00e9 que ele n\u00e3o s\u00f3 codifique a informa\u00e7\u00e3o, mas tamb\u00e9m nos diga a probabilidade de cada caracter\u00edstica.</p> <pre><code>class Encoder(nn.Module):\n    def __init__(self, latent_dim=20):\n        super(Encoder, self).__init__()\n        self.fc1 = nn.Linear(784, 512)\n        self.fc2 = nn.Linear(512, 256)\n        self.fc_mu = nn.Linear(256, latent_dim)\n        self.fc_logvar = nn.Linear(256, latent_dim)\n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n        h = self.relu(self.fc1(x.view(-1, 784)))\n        h = self.relu(self.fc2(h))\n        mu = self.fc_mu(h)\n        logvar = self.fc_logvar(h)\n        return mu, logvar\n</code></pre> <p>Basicamente: 784 (imagem achatada) \u2192 512 \u2192 256 \u2192 20 dimens\u00f5es latentes. Mantemos dois outputs: <code>mu</code> (m\u00e9dia) e <code>logvar</code> (log da vari\u00e2ncia).</p>"},{"location":"exercise4-VAE/main/#o-decodificador","title":"O Decodificador","text":"<p>Faz o processo inverso. Pega um vetor latente e tenta reconstruir a imagem original:</p> <pre><code>class Decoder(nn.Module):\n    def __init__(self, latent_dim=20):\n        super(Decoder, self).__init__()\n        self.fc1 = nn.Linear(latent_dim, 256)\n        self.fc2 = nn.Linear(256, 512)\n        self.fc3 = nn.Linear(512, 784)\n        self.relu = nn.ReLU()\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, z):\n        h = self.relu(self.fc1(z))\n        h = self.relu(self.fc2(h))\n        x_recon = self.sigmoid(self.fc3(h))\n        return x_recon\n</code></pre>"},{"location":"exercise4-VAE/main/#o-truque-de-reparametrizacao","title":"O \"Truque\" de Reparametriza\u00e7\u00e3o","text":"<p>Para conseguir treinar com gradiente descendente atrav\u00e9s da amostragem aleat\u00f3ria, precisamos fazer:</p> <pre><code>def reparameterize(self, mu, logvar):\n    std = torch.exp(0.5 * logvar)\n    eps = torch.randn_like(std)\n    z = mu + eps * std\n    return z\n</code></pre> <p>Em vez de simplesmente amostrar de uma distribui\u00e7\u00e3o, fazemos <code>z = \u03bc + \u03c3 \u00d7 \u03b5</code>. Dessa forma os gradientes conseguem passar pela opera\u00e7\u00e3o de amostragem.</p>"},{"location":"exercise4-VAE/main/#a-funcao-de-perda","title":"A fun\u00e7\u00e3o de perda","text":"<p>Tem dois termos que trabalham em tens\u00e3o:</p> <p>1. Reconstru\u00e7\u00e3o (BCE): Queremos que a imagem reconstru\u00edda seja parecida com a original. Usar BCE aqui faz sentido porque estamos trabalhando com pixels no intervalo [0,1].</p> <p>2. Diverg\u00eancia KL: For\u00e7a o espa\u00e7o latente a ser parecido com uma distribui\u00e7\u00e3o normal padr\u00e3o N(0,1). Isso \u00e9 o que torna o VAE especial - regulariza o espa\u00e7o latente.</p> <pre><code>def vae_loss(x_recon, x, mu, logvar):\n    bce = nn.BCELoss(reduction='sum')\n    reconstruction_loss = bce(x_recon, x.view(-1, 784))\n    kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n    return reconstruction_loss + kl_loss\n</code></pre> <p>A perda total \u00e9 simplesmente a soma dos dois. Existem abordagens mais sofisticadas (como beta-VAE), mas essa vers\u00e3o simples j\u00e1 funciona bem.</p>"},{"location":"exercise4-VAE/main/#dados-e-treinamento","title":"Dados e Treinamento","text":"<p>Usei MNIST - 60k imagens de treino, 10k de teste. Normalizei para [0,1] e dividi treino/valida\u00e7\u00e3o em 80/20.</p> <p>Treinei por 20 \u00e9pocas com Adam e learning rate 1e-3. Nada de especial, s\u00f3 o padr\u00e3o que funciona.</p>"},{"location":"exercise4-VAE/main/#os-resultados","title":"Os Resultados","text":""},{"location":"exercise4-VAE/main/#curva-de-perda","title":"Curva de Perda","text":"<p>A perda come\u00e7a alta (~160) e cai rapidamente nos primeiros 5 epochs. Depois desacelera e converge em torno de 100-102. O fato da valida\u00e7\u00e3o seguir o treino sem divergir muito \u00e9 bom - significa n\u00e3o est\u00e1 fazendo overfitting grave.</p>"},{"location":"exercise4-VAE/main/#reconstrucoes","title":"Reconstru\u00e7\u00f5es","text":"<p>Comparando original (primeira linha) com reconstru\u00eddo (segunda linha): os d\u00edgitos s\u00e3o claramente reconhec\u00edveis. N\u00e3o \u00e9 t\u00e3o n\u00edtido quanto um Autoencoder padr\u00e3o (que teria mais capacidade), mas tem esse aspecto \"suavizado\" t\u00edpico de VAE. </p> <p>A raz\u00e3o \u00e9 que o VAE sacrifica um pouco da qualidade de reconstru\u00e7\u00e3o em troca de um espa\u00e7o latente bem organizado. Sem isso, n\u00e3o conseguiria gerar amostras decentes.</p>"},{"location":"exercise4-VAE/main/#amostras-geradas","title":"Amostras Geradas","text":"<p>Aqui \u00e9 onde o VAE mostra seu diferencial. Amostrando vetores aleat\u00f3rios de uma distribui\u00e7\u00e3o normal, passando pelo decodificador, conseguimos gerar d\u00edgitos completamente novos que n\u00e3o estavam nos dados de treino.</p> <p>Todos os 16 aqui s\u00e3o reconhec\u00edveis como n\u00fameros. Nem sempre s\u00e3o perfeitos, mas mostram que o modelo realmente aprendeu a estrutura dos d\u00edgitos, n\u00e3o apenas memorizou.</p>"},{"location":"exercise4-VAE/main/#espaco-latente","title":"Espa\u00e7o Latente","text":"<p>Essa \u00e9 a parte mais legal. O espa\u00e7o latente de 20 dimens\u00f5es foi reduzido para 2D usando t-SNE. Cada cor \u00e9 um d\u00edgito diferente.</p> <p>Observe:</p> <ul> <li>Cada n\u00famero forma um cluster bem definido</li> <li>N\u00fameros similares ficam pr\u00f3ximos (3 e 8, 4 e 9)</li> <li>N\u00e3o h\u00e1 buracos ou discontinuidades abruptas</li> <li>Tudo forma um padr\u00e3o quase circular</li> </ul> <p>Se fosse um Autoencoder padr\u00e3o, teria clusters espalhados aleatoriamente. O que torna isso poss\u00edvel \u00e9 exatamente o termo KL divergence for\u00e7ando continuidade.</p>"},{"location":"exercise4-VAE/main/#comparacao-vae-vs-autoencoder-padrao","title":"Compara\u00e7\u00e3o: VAE vs Autoencoder Padr\u00e3o","text":"Aspecto VAE Autoencoder Espa\u00e7o Latente Estruturado e cont\u00ednuo Ca\u00f3tico Consegue gerar? Sim N\u00e3o Regulariza\u00e7\u00e3o Expl\u00edcita (KL) Nenhuma Reconstru\u00e7\u00e3o Um pouco desfocada Muito n\u00edtida <p>VAE \u00e9 um trade-off. Perdemos um pouco em qualidade de reconstru\u00e7\u00e3o mas ganhamos um espa\u00e7o latente interpret\u00e1vel e capacidade de gera\u00e7\u00e3o.</p>"},{"location":"projeto/main/","title":"Main","text":"<p>Aqui vai toda a documenta\u00e7\u00e3o do projeto, incluindo o que j\u00e1 foi feito e o que falta fazer.</p>"},{"location":"thisdocumentation/main/","title":"Main","text":""},{"location":"thisdocumentation/main/#pre-requisitos","title":"Pr\u00e9-requisitos","text":"<p>Antes de come\u00e7ar, certifique-se de que voc\u00ea possui os seguintes pr\u00e9-requisitos instalados em seu sistema:</p> <ul> <li>Git: Para clonar o reposit\u00f3rio.</li> </ul>"},{"location":"thisdocumentation/main/#instalando-o-python","title":"Instalando o Python","text":"LinuxmacOSWindows <p>Instale o Python 3.8 ou superior.</p> <pre><code>sudo apt install python3 python3-venv python3-pip\npython3 --version\n</code></pre> <p>Instale o Python 3.8 ou superior.</p> <pre><code>brew install python\npython3 --version\n</code></pre> <p>Instale o Python 3.13 ou superior. Baixe o instalador do site oficial do Python (https://www.python.org/downloads/) e execute-o. Certifique-se de marcar a op\u00e7\u00e3o \"Add Python to PATH\" durante a instala\u00e7\u00e3o.</p> <pre><code>python --version\n</code></pre>"},{"location":"thisdocumentation/main/#usage","title":"Usage","text":"<p>Para utilizar o c\u00f3digo deste reposit\u00f3rio, siga as instru\u00e7\u00f5es a seguir:</p> <p>Clone ou fork este reposit\u00f3rio:</p> <pre><code>git clone &lt;URL_DO_REPOSITORIO&gt;\n</code></pre> <p>Crie um ambiente virtual do Python:</p> Linux/macOSWindows <pre><code>python3 -m venv env\n</code></pre> <pre><code>python -m venv env\n</code></pre> <p>Ative o ambiente virtual (voc\u00ea deve fazer isso sempre que for executar algum script deste reposit\u00f3rio):</p> Linux/macOSWindows <pre><code>source ./env/bin/activate\n</code></pre> <pre><code>.\\env\\Scripts\\activate\n</code></pre> <p>Instale as depend\u00eancias com:</p> Linux/macOSWindows <pre><code>python3 -m pip install -r requirements.txt --upgrade\n</code></pre> <pre><code>python -m pip install -r requirements.txt --upgrade\n</code></pre>"},{"location":"thisdocumentation/main/#deployment","title":"Deployment","text":"<p>O material utiliza o mkdocs para gerar a documenta\u00e7\u00e3o. Para visualizar a documenta\u00e7\u00e3o, execute o comando:</p> <pre><code>mkdocs serve -o\n</code></pre> <p>Para subir ao GitHub Pages, execute o comando:</p> <pre><code>mkdocs gh-deploy\n</code></pre> <p>Esse reposit\u00f3rio possui um workflow do GitHub Actions que executa o comando <code>mkdocs gh-deploy</code> sempre que houver um push na branch <code>main</code>. Assim, n\u00e3o \u00e9 necess\u00e1rio executar esse comando manualmente. Toda vez que voc\u00ea fizer um push na branch <code>main</code>, a documenta\u00e7\u00e3o ser\u00e1 atualizada automaticamente no GitHub Pages.</p> <p>Aviso 1</p> <p>Para que o github actions funcione corretamente, \u00e9 necess\u00e1rio que o reposit\u00f3rio esteja configurado para que o bot <code>github-actions[bot]</code> tenha permiss\u00e3o de escrita. Voc\u00ea pode verificar isso nas configura\u00e7\u00f5es do reposit\u00f3rio, na se\u00e7\u00e3o \"Actions\" e depois em \"General\". Certifique-se de que a op\u00e7\u00e3o \"Workflow permissions\" esteja definida como \"Read and write permissions\".</p> <p></p> <p>Aviso 2</p> <p>Depois de publicar, caso n\u00e3o consiga acessar a p\u00e1gina, verifique se o github pages est\u00e1 configurado corretamente. V\u00e1 at\u00e9 as configura\u00e7\u00f5es do reposit\u00f3rio, na se\u00e7\u00e3o \"Pages\" e verifique se a branch <code>gh-pages</code> est\u00e1 selecionada como fonte. Se n\u00e3o estiver, selecione-a e salve as altera\u00e7\u00f5es.</p> <p></p> <p>Pay Attention</p> <p>No arquivo '<code>mkdocs.yml</code>, a se\u00e7\u00e3o <code>site_url</code> deve estar configurada corretamente para o seu reposit\u00f3rio. Por exemplo, se o seu reposit\u00f3rio estiver em <code>https://github.com/usuario/repositorio</code>, a se\u00e7\u00e3o <code>site_url</code> deve ser:</p> <pre><code>site_url: https://usuario.github.io/repositorio\n</code></pre> <p>Tamb\u00e9m, certifique-se de que a se\u00e7\u00e3o <code>repo_url</code> esteja configurada corretamente para o seu reposit\u00f3rio. Por exemplo:</p> <pre><code>repo_url: https://github.com/usuario/repositorio\n</code></pre>"}]}