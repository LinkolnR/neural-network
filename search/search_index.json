{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"P\u00e1gina Inicial","text":"Edi\u00e7\u00e3o <p>2025.2</p>"},{"location":"#repositorio","title":"Reposit\u00f3rio","text":""},{"location":"#lincoln-melo","title":"Lincoln Melo","text":""},{"location":"#entregas","title":"Entregas","text":"<ul> <li> Exerc\u00edcio 1 - Data 05/09/2025</li> <li> Exerc\u00edcio 2</li> <li> Exerc\u00edcio 3 </li> <li> Exerc\u00edcio 4 </li> <li> Projeto</li> </ul>"},{"location":"exercise1-data/main/","title":"Exerc\u00edcio 1","text":""},{"location":"exercise1-data/main/#exercicio-1-data","title":"Exerc\u00edcio 1 - Data","text":"<p>Entrega Referente ao exerc\u00edcio 1 do curso de Redes Neurais e Deep Learning</p>"},{"location":"exercise1-data/main/#relatorio-de-entrega","title":"Relat\u00f3rio de entrega!","text":""},{"location":"exercise1-data/main/#questao-1","title":"Quest\u00e3o 1","text":""},{"location":"exercise1-data/main/#11-e-12-geracao-de-dados-e-grafico-das-4-classes","title":"1.1 e 1.2 Gera\u00e7\u00e3o de dados e gr\u00e1fico das 4 classes","text":"<ul> <li>Class 0: Mean = \\([2, 3]\\), Standard Deviation = \\([0.8, 2.5]\\)</li> <li>Class 1: Mean = \\([5, 6]\\), Standard Deviation = \\([1.2, 1.9]\\)</li> <li>Class 2: Mean = \\([8, 1]\\), Standard Deviation = \\([0.9, 0.9]\\)</li> <li>Class 3: Mean = \\([15, 4]\\), Standard Deviation = \\([0.5, 2.0]\\)</li> </ul> <p>Primeiro \u00e9 necess\u00e1rio consegui gerar os dados, ent\u00e3o aqui utilizei a fun\u00e7\u00e3o do numpy para fazer a gera\u00e7\u00e3o dos n\u00fameros com uma seed fixa (n\u00famero escolhido foi o 37). A seguir temos o gr\u00e1fico plotado dos gr\u00e1ficos gerados.</p> <p>teste_simples:</p> 2025-09-21T06:51:31.203610 image/svg+xml Matplotlib v3.10.6, https://matplotlib.org/"},{"location":"exercise1-data/main/#13-analise-e-fronteiras","title":"1.3 An\u00e1lise e fronteiras","text":"<ol> <li> <ol> <li> <ul> <li>Classe 3 apresenta uma maior diferen\u00e7a, o que permite com que separar mais facilmente na regi\u00e3o da direita do gr\u00e1fico.</li> <li>Classe 0, 1 e 2 est\u00e3o bem mais pr\u00f3ximas, o que dificulta uma separa\u00e7\u00e3o entre elas. Vendo o gr\u00e1fico \u00e9 poss\u00edvel perceber que \u00e9 poss\u00edvel tra\u00e7ar uma reta dividindo a classe 2 das classes 0 e 1. Como ela tamb\u00e9m tem um desvio menor a identifica\u00e7\u00e3o dela se torna mais f\u00e1cil.</li> <li>J\u00e1 as entre as classes 0 e 1 existe uma superposi\u00e7\u00e3o maior, por conta da dispers\u00e3o, ent\u00e3o n\u00e3o \u00e9 poss\u00edvel tra\u00e7ar uma linha que dividida perfeitamente elas.</li> </ul> </li> <li> <p>N\u00e3o, \u00e9 imposs\u00edvel separar todas as classes perfeitamente com fronteiras lineares. A classe 3 \u00e9 separ\u00e1vel linearmente das outras, por\u00e9m as classes 0 e 1 tem m\u00falticas sobreposi\u00e7\u00f5es. Ou seja, as fronteiras/bordas precisariam de curvas.</p> </li> <li></li> </ol> </li> </ol> 2025-09-21T06:51:31.813033 image/svg+xml Matplotlib v3.10.6, https://matplotlib.org/"},{"location":"exercise1-data/main/#questao-2","title":"Quest\u00e3o 2","text":""},{"location":"exercise1-data/main/#11","title":"1.1","text":"<p>Realizando a gera\u00e7\u00e3o das amostras e redu\u00e7\u00e3o de dimensionalidade utilizando t\u00e9cnicas de PCA para manter as duas dimens\u00f5es que carregam mais informa\u00e7\u00f5es. Assim, temos o seguinte gr\u00e1fico:</p> <p> 2025-09-21T06:51:32.406371 image/svg+xml Matplotlib v3.10.6, https://matplotlib.org/    Aqui podemos perceber que a Classe A (vermelha): - Se concentra na parte direita do gr\u00e1fico (PC1 positivo) - Centro pr\u00f3ximo da coordenada (2,0) no gr\u00e1fico</p> <p>Sobre a Classe B (azul): - Se concentra na parte esquerda do gr\u00e1fico (PC1 negativo) - Centro pr\u00f3ximo da coordenada (-2,0) no gr\u00e1fico</p> <p>Tem uma regi\u00e3o central na qual existe uma sobreposi\u00e7\u00e3o onde as duas classes se misturam Mesmo com uma tend\u00eancia de separa\u00e7\u00e3o, (um mais negativo e um mais positivo ao longo de PC1),  n\u00e3o existe uma linha reta para poder dividir as duas classes completamente. Ent\u00e3o a linha de fronteira, de divis\u00e3o n\u00e3o \u00e9 trivial, seria necess\u00e1rio uma fun\u00e7\u00e3o mais complexa com curvas. Por isso, seria necess\u00e1rio redes neurais para conseguir tra\u00e7ar essa separa\u00e7\u00e3o complexa de uma forma que realmente conseguisse dividir bem em duas ou  mais regi\u00f5es.</p>"},{"location":"exercise1-data/main/#questao-3","title":"Quest\u00e3o 3","text":""},{"location":"exercise1-data/main/#31-carregamento-e-descricao-dos-dados","title":"3.1 Carregamento e Descri\u00e7\u00e3o dos Dados","text":"<p>O dataset Spaceship Titanic \u00e9 um problema de classifica\u00e7\u00e3o bin\u00e1ria para prever se um passageiro foi transportado para uma dimens\u00e3o alternativa durante um acidente espacial.</p> <p>Features Identificadas:</p> <p>Num\u00e9ricas (6 features): - <code>Age</code>: Idade do passageiro (0-80 anos) - <code>RoomService</code>, - <code>FoodCourt</code> - <code>ShoppingMall</code> - <code>Spa</code> - <code>VRDeck</code>: Gastos em cr\u00e9ditos gal\u00e1cticos</p> <p>Categ\u00f3ricas (7 features): - <code>HomePlanet</code>: Planeta de origem (Earth, Europa, Mars) - <code>CryoSleep</code>: Em sono criog\u00eanico (True/False) - <code>Destination</code>: Destino da viagem (3 planetas) - <code>VIP</code>: Status VIP (True/False) - <code>Cabin</code>: Localiza\u00e7\u00e3o da cabine (formato complexo) - <code>Name</code>, <code>PassengerId</code>: Identificadores</p> <p>An\u00e1lise Detalhada dos valores faltantes:</p> 2025-09-21T06:51:35.589445 image/svg+xml Matplotlib v3.10.6, https://matplotlib.org/"},{"location":"exercise1-data/main/#32-pre-processamento-completo-com-justificativas","title":"3.2 Pr\u00e9-processamento Completo com Justificativas","text":""},{"location":"exercise1-data/main/#estrategia-1-tratamento-de-dados-faltantes","title":"Estrat\u00e9gia 1: Tratamento de Dados Faltantes","text":"<p>Baseado no gr\u00e1fico acima, identificamos dados faltantes em 9 features. Vamos aqui analisar as estrat\u00e9gias para contornar esses valores faltantes.</p> <p>Para Features Num\u00e9ricas:</p> <ul> <li> <p>Age (Idade): ~15.0% faltantes \u2192 Imputa\u00e7\u00e3o pela mediana</p> <ul> <li>Assim conseguimos preservar a distribui\u00e7\u00e3o da idade e dessa forma, tamb\u00e9m evitando um bias no gradiente na etapa de backpropagation</li> </ul> </li> <li> <p>RoomService (Servi\u00e7o Quarto): ~10.4% faltantes \u2192 Preenchimento com zero</p> <ul> <li>Sendo uma aus\u00eancia do registro pode fazer sentido que possa ser igual a 0 cr\u00e9ditos gastos. Al\u00e9m disso, como estamos utilizando a tanh como fun\u00e7\u00e3o de ativa\u00e7\u00e3o, o elemento 0 \u00e9 neutro e n\u00e3o interfere no gradiente.</li> </ul> </li> <li> <p>FoodCourt (Pra\u00e7a Alimenta\u00e7\u00e3o): ~10.2% faltantes \u2192 Preenchimento com zero</p> <ul> <li>Mesma l\u00f3gica do anterior, provavelmente a pessoa n\u00e3o utilizou o servi\u00e7o (0 cr\u00e9ditos) e com o valor 0 n\u00e3o distorce os padr\u00f5es de gasto</li> </ul> </li> <li> <p>ShoppingMall (Shopping): ~10.7% faltantes \u2192 Preenchimento com zero</p> <ul> <li>Mantendo na mesma l\u00f3gica, a pessoa provavelmente n\u00e3o consumiu nada, ent\u00e3o 0. Da mesma forma das duas anteriores.</li> </ul> </li> <li> <p>Spa: ~11.6% faltantes \u2192 Preenchimento com zero</p> <ul> <li>0 = n\u00e3o utilizou servi\u00e7os de spa. 0 N\u00e3o afeta padr\u00f5es e os mantem uniforme</li> </ul> </li> <li> <p>VRDeck (Deck VR): ~11.2% faltantes \u2192 Preenchimento com zero</p> <ul> <li>Novamente 0 por n\u00e3o ter registro pode indicar n\u00e3o ter um uso de fato. E ser neutro para a fun\u00e7\u00e3o de ativa\u00e7\u00e3o.</li> </ul> </li> </ul> <p>Para Features Categ\u00f3ricas:</p> <ul> <li> <p>HomePlanet (Planeta Origem): ~1.1% faltantes \u2192 Imputa\u00e7\u00e3o pela moda</p> <ul> <li>Para preservar distribui\u00e7\u00e3o original e evitar a cria\u00e7\u00e3o de uma outra categoria que s\u00f3 representaria 1%</li> </ul> </li> <li> <p>Cabin (Cabine): ~1.9% faltantes \u2192 Imputa\u00e7\u00e3o pela moda</p> <ul> <li>Novamente, evitar adicionar uma nova categoria e preservando a distribui\u00e7\u00e3o.</li> </ul> </li> <li> <p>Destination (Destino): ~1.1% faltantes \u2192 Imputa\u00e7\u00e3o pela moda</p> <ul> <li>Preserva\u00e7\u00e3o da distribui\u00e7\u00e3o sem adi\u00e7\u00e3o de novas categorias</li> </ul> </li> </ul>"},{"location":"exercise1-data/main/#estrategia-2-encoding-de-variaveis-categoricas","title":"Estrat\u00e9gia 2: Encoding de Vari\u00e1veis Categ\u00f3ricas","text":"<p>One-Hot Encoding para categ\u00f3ricas nominais: - <code>HomePlanet</code> \u2192 3 colunas bin\u00e1rias - <code>Destination</code> \u2192 3 colunas bin\u00e1rias - <code>Cabin</code> (Deck/Side) \u2192 m\u00faltiplas colunas bin\u00e1rias</p> <p>Label Encoding para categ\u00f3ricas booleanas: - <code>CryoSleep</code>: False\u21920, True\u21921 - <code>VIP</code>: False\u21920, True\u21921</p> <p>Justificativa: One-hot evita ordinality artificial, essencial para redes neurais interpretarem categorias independentemente.</p>"},{"location":"exercise1-data/main/#estrategia-3-normalizacao-otimizada-para-funcao-tanh","title":"Estrat\u00e9gia 3: Normaliza\u00e7\u00e3o Otimizada para Fun\u00e7\u00e3o Tanh","text":"<p>M\u00e9todo Escolhido: Z-score normalization</p> <p>Transforma\u00e7\u00e3o Matem\u00e1tica: <pre><code>X_normalized = (X - \u03bc) / \u03c3\n</code></pre></p> <p>Justificativas Te\u00f3ricas:</p> <ol> <li>Regi\u00e3o Ativa da Tanh:</li> <li><code>tanh(x)</code> \u00e9 mais sens\u00edvel em [-2, +2]</li> <li> <p>Utilizando o Z-score centraliza dados em \u03bc=0</p> </li> <li> <p>Converg\u00eancia Otimizada:</p> </li> <li>Gradientes balanceados entre features</li> <li> <p>Evita satura\u00e7\u00e3o da fun\u00e7\u00e3o tanh</p> </li> <li> <p>Preven\u00e7\u00e3o de Problemas:</p> </li> <li>Converg\u00eancia Lenta: Features dominantes mascararam outras</li> </ol>"},{"location":"exercise1-data/main/#33-visualizacoes-demonstrando-o-impacto-tive-problema-com-os-graficos-entao-retirei-essa-parte","title":"3.3 Visualiza\u00e7\u00f5es Demonstrando o Impacto (Tive problema com os gr\u00e1ficos, ent\u00e3o retirei essa parte)","text":""},{"location":"exercise2-perceptron/main/","title":"Perceptron","text":""},{"location":"exercise2-perceptron/main/#parte-1-implementacao-perceptron","title":"Parte 1: Implementa\u00e7\u00e3o Perceptron","text":""},{"location":"exercise2-perceptron/main/#estrutura-do-codigo","title":"Estrutura do C\u00f3digo","text":"<p>Criaremos uma classe <code>Perceptron</code></p> <pre><code>import numpy as np\n\nclass Perceptron:\n    \"\"\"\n    Implementa\u00e7\u00e3o do Perceptron\n\n    Par\u00e2metros\n    ----------\n    learning_rate : float\n        A taxa de aprendizado (entre 0.0 e 1.0)\n    n_iters : int\n        O n\u00famero de passagens (\u00e9pocas)\n\n    Atributos\n    ---------\n    weights\n    bias\n    \"\"\"\n    def __init__(self, learning_rate=0.01, n_iters=10):\n        self.learning_rate = learning_rate\n        self.n_iters = n_iters\n        self.weights = None\n        self.bias = None\n\n    def _activation_function(self, x):\n        \"\"\"\n        Fun\u00e7\u00e3o de Ativa\u00e7\u00e3o Degrau (Heaviside Step Function).\n        Retorna 1 se x &gt;= 0, sen\u00e3o retorna 0.\n        \"\"\"\n        return np.where(x &gt;= 0, 1, 0)\n\n    def fit(self, X, y):\n        \"\"\"\n        Ajusta o modelo aos dados de treinamento.\n\n        Par\u00e2metros\n        ----------\n        X : array-like, shape = [n_samples, n_features]\n            Vetor de treinamento, onde n_samples \u00e9 o n\u00famero de amostras\n            e n_features \u00e9 o n\u00famero de caracter\u00edsticas.\n        y : array-like, shape = [n_samples]\n            Vetor com os r\u00f3tulos (labels) alvo.\n        \"\"\"\n        n_samples, n_features = X.shape\n\n        # Inicializa\u00e7\u00e3o dos pesos e bias\n        self.weights = np.zeros(n_features)\n        self.bias = 0\n\n        y_ = np.array(y)\n\n        for _ in range(self.n_iters):\n            for idx, x_i in enumerate(X):\n                linear_output = np.dot(x_i, self.weights) + self.bias\n                y_predicted = self._activation_function(linear_output)\n                # C\u00e1lculo do Erro e Atualiza\u00e7\u00e3o dos Pesos\n                # A atualiza\u00e7\u00e3o s\u00f3 acontece se houver erro\n                error = y_[idx] - y_predicted\n                update = self.learning_rate * error\n\n                self.weights += update * x_i\n                self.bias += update\n\n    def predict(self, X):\n        linear_output = np.dot(X, self.weights) + self.bias\n        y_predicted = self._activation_function(linear_output)\n        return y_predicted\n</code></pre>"},{"location":"exercise2-perceptron/main/#parte-2-exercicios","title":"Parte 2: Exerc\u00edcios","text":""},{"location":"exercise2-perceptron/main/#exercicio-1-dados-linearmente-separaveis","title":"Exerc\u00edcio 1: Dados Linearmente Separ\u00e1veis","text":""},{"location":"exercise2-perceptron/main/#1-geracao-dos-dados","title":"1. Gera\u00e7\u00e3o dos Dados","text":"<p>Utilizamos o script <code>data_generation.py</code> para criar duas classes de pontos que est\u00e3o distantes uma da outra, garantindo a separabilidade linear. Os par\u00e2metros chave s\u00e3o:</p> <pre><code># data_generation.py\nmean0 = [1.5, 1.5]\ncov0 = [[0.5, 0], [0, 0.5]]\nmean1 = [5, 5]\ncov1 = [[0.5, 0], [0, 0.5]]\n</code></pre> <p>Ao executar o script, obtemos a seguinte visualiza\u00e7\u00e3o:</p> <p> 2025-09-21T06:51:36.405688 image/svg+xml Matplotlib v3.10.6, https://matplotlib.org/ <p> </p> <p>Como podemos ver no gr\u00e1fico, os dois grupos (vermelho e azul) n\u00e3o se misturam, representando um caso ideal para o Perceptron.</p>"},{"location":"exercise2-perceptron/main/#2-treinamento","title":"2. Treinamento","text":"<p>No script <code>test_perceptron.py</code>, instanciamos nosso Perceptron e o treinamos com 80% dos dados gerados, utilizando o m\u00e9todo <code>fit</code>.</p> <pre><code># Trecho de test_perceptron.py\n# ...\np = Perceptron(learning_rate=0.01, n_iters=10)\np.fit(X_train, y_train)\n</code></pre>"},{"location":"exercise2-perceptron/main/#3-avaliacao-evaluation","title":"3. Avalia\u00e7\u00e3o (Evaluation)","text":"<p>Ap\u00f3s o treinamento, usamos os 20% de dados restantes (o conjunto de teste) para verificar o qu\u00e3o bem o modelo generaliza para dados n\u00e3o vistos.</p> <pre><code># Trecho de test_perceptron.py\npredictions = p.predict(X_test)\nprint(f\"Acur\u00e1cia do Perceptron no conjunto de teste: {accuracy(y_test, predictions):.2f}\")\n</code></pre> <p>Resultado Obtido: Ao executar o script, a acur\u00e1cia impressa no terminal \u00e9 1.00 (ou 100%).</p>"},{"location":"exercise2-perceptron/main/#4-analise","title":"4. An\u00e1lise","text":"<p>O resultado de 100% de acur\u00e1cia confirma o Teorema da Converg\u00eancia do Perceptron, que afirma que o algoritmo sempre encontrar\u00e1 uma solu\u00e7\u00e3o (uma linha de separa\u00e7\u00e3o) em um n\u00famero finito de passos, desde que os dados sejam linearmente separ\u00e1veis. </p> <p>Podemos visualizar essa solu\u00e7\u00e3o plotando a fronteira de decis\u00e3o que o Perceptron aprendeu:</p> <p>Acur\u00e1cia do Perceptron no conjunto de teste: 1.00</p> <p> 2025-09-21T06:51:37.630755 image/svg+xml Matplotlib v3.10.6, https://matplotlib.org/ <p> </p> <p>A linha preta representa a fronteira final encontrada pelo algoritmo. Qualquer ponto de um lado da linha \u00e9 classificado como uma classe, e qualquer ponto do outro lado \u00e9 classificado como a outra, separando perfeitamente os dados, que s\u00e3o claramente linearmente separ\u00e1veis.</p>"},{"location":"exercise2-perceptron/main/#exercicio-2-dados-com-sobreposicao-nao-linearmente-separaveis","title":"Exerc\u00edcio 2: Dados com Sobreposi\u00e7\u00e3o (N\u00e3o Linearmente Separ\u00e1veis)","text":""},{"location":"exercise2-perceptron/main/#1-geracao-dos-dados_1","title":"1. Gera\u00e7\u00e3o dos Dados","text":"<p>Utilizamos o script <code>data_generation2.py</code> para criar duas classes de pontos com m\u00e9dias mais pr\u00f3ximas e maior vari\u00e2ncia, o que causa uma sobreposi\u00e7\u00e3o entre elas. Os par\u00e2metros chave s\u00e3o:</p> <pre><code># data_generation2.py\nmean0 = [3, 3]\ncov0 = [[1.5, 0], [0, 1.5]]\nmean1 = [4, 4]\ncov1 = [[1.5, 0], [0, 1.5]]\n</code></pre> <p>Ao executar o script, obtemos a seguinte visualiza\u00e7\u00e3o, onde a mistura entre os pontos vermelhos e azuis \u00e9 clara:</p> <p> 2025-09-21T06:51:39.860136 image/svg+xml Matplotlib v3.10.6, https://matplotlib.org/ <p> </p>"},{"location":"exercise2-perceptron/main/#2-treinamento_1","title":"2. Treinamento","text":"<p>O treinamento, realizado pelo script <code>test_perceptron2.py</code>, segue o mesmo procedimento. O Perceptron tentar\u00e1 encontrar a melhor linha reta poss\u00edvel para separar as duas classes.</p> <pre><code># Trecho de test_perceptron2.py\np = Perceptron(learning_rate=0.01, n_iters=10)\np.fit(X_train, y_train)\n</code></pre>"},{"location":"exercise2-perceptron/main/#3-avaliacao-evaluation_1","title":"3. Avalia\u00e7\u00e3o (Evaluation)","text":"<p>A avalia\u00e7\u00e3o \u00e9 feita da mesma forma, usando o conjunto de teste. No entanto, o resultado esperado \u00e9 diferente.</p> <pre><code># test_perceptron2.py\npredictions = p.predict(X_test)\nprint(f\"Acur\u00e1cia do Perceptron no conjunto de teste com sobreposi\u00e7\u00e3o: {accuracy(y_test, predictions):.2f}\")\n</code></pre> <p>Resultado Obtido: Ao executar o script, a acur\u00e1cia impressa fica em torno de 0.66 (ou 66%). Este valor pode variar ligeiramente devido \u00e0 aleatoriedade na divis\u00e3o treino-teste, mas nunca chegar\u00e1 a 100%.</p>"},{"location":"exercise2-perceptron/main/#4-analise_1","title":"4. An\u00e1lise","text":"<p>A acur\u00e1cia abaixo de 100% demonstra a principal limita\u00e7\u00e3o do Perceptron: ele s\u00f3 pode encontrar solu\u00e7\u00f5es perfeitas para problemas linearmente separ\u00e1veis. Como os dados se sobrep\u00f5em, n\u00e3o existe uma \u00fanica linha reta que consiga dividir as duas classes sem cometer erros.</p> <p>O algoritmo n\u00e3o \"converge\" para uma solu\u00e7\u00e3o sem erros. Em vez disso, a fronteira de decis\u00e3o pode oscilar um pouco durante o treinamento. O resultado final, ap\u00f3s o n\u00famero definido de itera\u00e7\u00f5es, \u00e9 a melhor tentativa do Perceptron de minimizar os erros com uma \u00fanica linha.</p> <p>A fronteira de decis\u00e3o, visualizada ao executar o teste, mostra essa tentativa:</p> <p>Acur\u00e1cia do Perceptron no conjunto de teste com sobreposi\u00e7\u00e3o: 0.66</p> <p> 2025-09-21T06:51:43.480360 image/svg+xml Matplotlib v3.10.6, https://matplotlib.org/ <p> </p> <p>A linha preta corta atrav\u00e9s da \u00e1rea de sobreposi\u00e7\u00e3o, classificando incorretamente alguns pontos de ambas as classes que est\u00e3o do lado \"errado\" da fronteira. Este \u00e9 o comportamento esperado e ilustra por que modelos mais complexos, como Redes Neurais de M\u00faltiplas Camadas (MLPs), s\u00e3o necess\u00e1rios para resolver problemas n\u00e3o-lineares.</p>"},{"location":"exercise3-MLP/main/","title":"Exerc\u00edcio 3","text":""},{"location":"exercise3-MLP/main/#exercicio-2-classificacao-binaria-com-dados-sinteticos-e-mlp-do-zero","title":"Exerc\u00edcio 2 \u2014 Classifica\u00e7\u00e3o Bin\u00e1ria com Dados Sint\u00e9ticos e MLP do Zero","text":"<p>Este relat\u00f3rio descreve a gera\u00e7\u00e3o do conjunto de dados, a implementa\u00e7\u00e3o de uma MLP (Multi-Layer Perceptron) do zero usando apenas NumPy para opera\u00e7\u00f5es matriciais, o processo de treinamento, e a avalia\u00e7\u00e3o final. Todo o c\u00f3digo foi implementado em <code>docs/exercise3-MLP/mlp.py</code>.</p>"},{"location":"exercise3-MLP/main/#objetivo","title":"Objetivo","text":"<ul> <li>Tarefa: Classificar dois grupos (bin\u00e1rio) com 2 features para facilitar a visualiza\u00e7\u00e3o.</li> <li>Dados: 1000 amostras, com 1 cluster para a classe 0 e 2 clusters para a classe 1, garantindo um cen\u00e1rio ligeiramente desafiador e separ\u00e1vel.</li> </ul>"},{"location":"exercise3-MLP/main/#geracao-de-dados-make_classification","title":"Gera\u00e7\u00e3o de Dados (make_classification)","text":"<p>Para obter um n\u00famero diferente de clusters por classe (1 para a classe 0 e 2 para a classe 1), a fun\u00e7\u00e3o <code>make_classification</code> foi chamada duas vezes e os subconjuntos foram combinados: - Chamada A: <code>weights=[1.0, 0.0]</code>, <code>n_clusters_per_class=1</code> \u2192 gera somente a classe 0, com 1 cluster. - Chamada B: <code>weights=[0.0, 1.0]</code>, <code>n_clusters_per_class=2</code> \u2192 gera somente a classe 1, com 2 clusters.</p> <p>Par\u00e2metros principais utilizados: - <code>n_features=2</code>, <code>n_informative=2</code>, <code>n_redundant=0</code> (visualiz\u00e1vel e informativo), - <code>class_sep\u22481.5\u20131.6</code> e <code>flip_y\u22480.01</code> (separ\u00e1vel com ru\u00eddo leve), - <code>random_state/seed=42</code> para reprodutibilidade.</p> <p>Ap\u00f3s gerar as amostras, os dados s\u00e3o embaralhados, divididos em treino (80%) e teste (20%), e padronizados (m\u00e9dia 0, desvio 1) usando apenas as estat\u00edsticas do conjunto de treino.</p>"},{"location":"exercise3-MLP/main/#mlp-do-zero-sem-frameworks-de-dl","title":"MLP do Zero (sem frameworks de DL)","text":"<p>Implementada em <code>mlp.py</code>, utilizando apenas NumPy para vetores e matrizes. Os componentes foram codificados manualmente (ativa\u00e7\u00e3o, loss, forward, backward e atualiza\u00e7\u00e3o de par\u00e2metros), conforme exigido.</p>"},{"location":"exercise3-MLP/main/#arquitetura","title":"Arquitetura","text":"<ul> <li>Entrada: 2 features.</li> <li>Camadas ocultas: por padr\u00e3o, <code>[16, 16]</code> neur\u00f4nios com ReLU.</li> <li>Sa\u00edda: 1 neur\u00f4nio com sigmoid para probabilidade da classe 1.</li> </ul>"},{"location":"exercise3-MLP/main/#inicializacao-de-pesos","title":"Inicializa\u00e7\u00e3o de Pesos","text":"<ul> <li>Camadas ocultas (ReLU): He (desvio padr\u00e3o \u221a(2/fan_in)).</li> <li>Camada de sa\u00edda (sigmoid): Xavier (desvio padr\u00e3o \u221a(1/fan_in)). Essas escolhas estabilizam a escala dos sinais no in\u00edcio do treinamento e costumam acelerar a converg\u00eancia.</li> </ul>"},{"location":"exercise3-MLP/main/#funcoes-de-ativacao-justificativa","title":"Fun\u00e7\u00f5es de Ativa\u00e7\u00e3o \u2014 justificativa","text":"<ul> <li>ReLU nas ocultas: simples, eficiente e reduz problemas de gradientes muito pequenos em compara\u00e7\u00e3o ao sigmoid/tanh.</li> <li>Sigmoid na sa\u00edda: adequada para classifica\u00e7\u00e3o bin\u00e1ria, pois modela P(y=1|x) \u2208 (0,1).</li> </ul>"},{"location":"exercise3-MLP/main/#funcao-de-perda-justificativa","title":"Fun\u00e7\u00e3o de Perda \u2014 justificativa","text":"<ul> <li>Binary Cross-Entropy (BCE): \u00e9 a perda can\u00f4nica para classifica\u00e7\u00e3o bin\u00e1ria probabil\u00edstica. \u00c9 consistente com a sa\u00edda sigmoid e fornece gradientes bem comportados.</li> </ul>"},{"location":"exercise3-MLP/main/#otimizador-justificativa","title":"Otimizador \u2014 justificativa","text":"<ul> <li>Descida do Gradiente (taxa de aprendizado padr\u00e3o <code>0.05</code>): simples, transparente e suficiente para este problema sint\u00e9tico. Taxas diferentes podem ser testadas via CLI para balancear estabilidade e velocidade de converg\u00eancia.</li> </ul>"},{"location":"exercise3-MLP/main/#treinamento","title":"Treinamento","text":"<ul> <li>N\u00famero de \u00e9pocas padr\u00e3o: <code>300</code> (ajust\u00e1vel via CLI).</li> <li>Monitoramento: perda de treino por \u00e9poca (BCE). Ao final, s\u00e3o gerados gr\u00e1ficos e m\u00e9tricas.</li> </ul>"},{"location":"exercise3-MLP/main/#avaliacao","title":"Avalia\u00e7\u00e3o","text":"<ul> <li>Acur\u00e1cia no conjunto de teste (20%).</li> <li>Matriz de confus\u00e3o (TP, TN, FP, FN).</li> <li>Fronteira de decis\u00e3o em 2D para visualiza\u00e7\u00e3o.</li> </ul> <p>Gr\u00e1ficos salvos na mesma pasta do script: - <code>training_loss.png</code> \u2014 curva de perda de treinamento. - <code>decision_boundary.png</code> \u2014 fronteira de decis\u00e3o com pontos de treino e teste. - <code>confusion_matrix.png</code> \u2014 matriz de confus\u00e3o.</p> <p>M\u00e9tricas salvas em <code>metrics.txt</code>.</p>"},{"location":"exercise3-MLP/main/#como-executar-windows-powershell","title":"Como Executar (Windows PowerShell)","text":"<p>Requisitos: ambiente Python do projeto (a pasta <code>env</code> j\u00e1 cont\u00e9m as depend\u00eancias, incluindo scikit-learn e matplotlib).</p> <p>1) Executar o treinamento com os padr\u00f5es propostos: <pre><code>.\\env\\Scripts\\python.exe docs\\exercise3-MLP\\mlp.py --samples 1000 --epochs 300 --lr 0.05 --hidden 16 16 --seed 42 --class_sep 1.6 --flip_y 0.01\n</code></pre></p> <p>2) Ap\u00f3s a execu\u00e7\u00e3o, ver os resultados: <pre><code>Get-Content docs\\exercise3-MLP\\metrics.txt\n</code></pre></p> <p>Os arquivos <code>training_loss.png</code>, <code>decision_boundary.png</code> e <code>confusion_matrix.png</code> estar\u00e3o em <code>docs\\exercise3-MLP\\</code> e ser\u00e3o automaticamente publicados no GitHub Pages junto com esta p\u00e1gina.</p>"},{"location":"exercise3-MLP/main/#resultados-esperados-diretriz","title":"Resultados Esperados (diretriz)","text":"<p>Com <code>class_sep</code> por volta de 1.5\u20131.6 e <code>flip_y=0.01</code>, espera-se que a MLP alcance acur\u00e1cia acima de 0.90 no conjunto de teste, mantendo uma fronteira de decis\u00e3o que respeita as 3 regi\u00f5es (1 cluster da classe 0 e 2 da classe 1). Valores exatos podem variar com o seed, taxa de aprendizado e n\u00famero de \u00e9pocas.</p>"},{"location":"exercise3-MLP/main/#reprodutibilidade","title":"Reprodutibilidade","text":"<ul> <li><code>seed</code> fixo (padr\u00e3o 42) em todas as partes do pipeline.</li> <li>Padroniza\u00e7\u00e3o feita somente com estat\u00edsticas do treino.</li> </ul>"},{"location":"exercise3-MLP/main/#arquivos-e-principais-funcoes","title":"Arquivos e Principais Fun\u00e7\u00f5es","text":"<ul> <li>C\u00f3digo: <code>docs/exercise3-MLP/mlp.py</code></li> <li><code>generate_uneven_cluster_data(...)</code>: gera 1 cluster para a classe 0 e 2 para a classe 1 combinando duas chamadas de <code>make_classification</code>.</li> <li><code>MLPBinaryClassifier</code>: implementa\u00e7\u00e3o da MLP com forward, backward e update manual.</li> <li>Plots: perda, fronteira de decis\u00e3o e matriz de confus\u00e3o.</li> <li>CLI: permite customizar amostras, \u00e9pocas, lr, camadas ocultas, seed, class_sep, flip_y.</li> </ul>"},{"location":"exercise3-MLP/main/#conformidade-com-as-regras-do-exercicio","title":"Conformidade com as Regras do Exerc\u00edcio","text":"<ul> <li>Apenas NumPy foi usado para as opera\u00e7\u00f5es matriciais do modelo (ativa\u00e7\u00e3o, perda, gradientes e forward/backward foram implementados manualmente).</li> <li><code>scikit-learn</code> foi utilizado somente para a gera\u00e7\u00e3o dos dados (permitido).</li> </ul>"},{"location":"exercise3-MLP/main/#colaboracao-com-ia","title":"Colabora\u00e7\u00e3o com IA","text":"<p>Esta implementa\u00e7\u00e3o e documenta\u00e7\u00e3o contaram com suporte de uma IA para acelerar a escrita do c\u00f3digo e da an\u00e1lise. Todo o conte\u00fado foi revisado para garantir entendimento e ader\u00eancia \u00e0s regras do exerc\u00edcio.</p>"},{"location":"exercise3-MLP/main/#exercicio-3-classificacao-multiclasse-com-dados-sinteticos-e-mlp-reutilizavel","title":"Exerc\u00edcio 3 \u2014 Classifica\u00e7\u00e3o Multiclasse com Dados Sint\u00e9ticos e MLP Reutiliz\u00e1vel","text":"<p>Neste exerc\u00edcio, aumentamos a complexidade para 3 classes e 4 features, preservando a filosofia do Exerc\u00edcio 2: a MLP \u00e9 implementada do zero, reutilizando a mesma estrutura base (apenas adaptando hiperpar\u00e2metros e a camada/ loss de sa\u00edda).</p>"},{"location":"exercise3-MLP/main/#geracao-de-dados-3-classes-clusters-desiguais","title":"Gera\u00e7\u00e3o de Dados (3 classes, clusters desiguais)","text":"<p>Usamos <code>make_classification</code> tr\u00eas vezes, uma para cada classe, for\u00e7ando que cada chamada gere apenas uma das classes (via <code>weights</code>) e definindo um n\u00famero diferente de clusters por classe: - Classe 0: 2 clusters - Classe 1: 3 clusters - Classe 2: 4 clusters</p> <p>Par\u00e2metros: <code>n_features=4</code>, <code>n_informative=4</code>, <code>n_redundant=0</code>, <code>class_sep\u22481.6</code>, <code>flip_y\u22480.01</code>, <code>seed=42</code>. As tr\u00eas partes s\u00e3o concatenadas e embaralhadas. Depois, dividimos em treino (80%) e teste (20%) e padronizamos com estat\u00edsticas do treino.</p>"},{"location":"exercise3-MLP/main/#mlp-multiclasse-reuso-do-exercicio-2","title":"MLP Multiclasse (Reuso do Exerc\u00edcio 2)","text":"<ul> <li>Estrutura id\u00eantica: camadas ocultas com ReLU, inicializa\u00e7\u00e3o He nas ocultas e Xavier na sa\u00edda.</li> <li>Diferen\u00e7as necess\u00e1rias para multiclasse: camada de sa\u00edda com softmax (3 neur\u00f4nios) e categorical cross-entropy como fun\u00e7\u00e3o de perda.</li> <li>Otimizador: Descida do Gradiente com taxa padr\u00e3o <code>0.05</code> (ajust\u00e1vel via CLI).</li> </ul> <p>Arquitetura sugerida: <code>[32, 32]</code> neur\u00f4nios nas ocultas. Entrada com 4 features, sa\u00edda com 3 classes.</p>"},{"location":"exercise3-MLP/main/#treinamento-e-avaliacao","title":"Treinamento e Avalia\u00e7\u00e3o","text":"<ul> <li>\u00c9pocas padr\u00e3o: <code>400</code>.</li> <li>M\u00e9trica: acur\u00e1cia no conjunto de teste.</li> <li>Visualiza\u00e7\u00f5es:</li> <li><code>training_loss_ex3.png</code> \u2014 curva da perda categ\u00f3rica.</li> <li><code>pca_scatter_ex3.png</code> \u2014 proje\u00e7\u00e3o PCA 2D colorida pela classe prevista (treino e teste).</li> <li><code>confusion_matrix_ex3.png</code> \u2014 matriz de confus\u00e3o multiclasse.</li> <li>M\u00e9tricas salvas em <code>metrics_ex3.txt</code>.</li> </ul>"},{"location":"exercise3-MLP/main/#como-executar-windows-powershell_1","title":"Como Executar (Windows PowerShell)","text":"<pre><code>.\\env\\Scripts\\python.exe docs\\exercise3-MLP\\mlp_ex3.py --samples 1500 --epochs 400 --lr 0.05 --hidden 32 32 --seed 42 --class_sep 1.6 --flip_y 0.01 --clusters 2 3 4\n</code></pre> <p>Depois, visualize as m\u00e9tricas: <pre><code>Get-Content docs\\exercise3-MLP\\metrics_ex3.txt\n</code></pre></p>"},{"location":"exercise3-MLP/main/#resultados-esperados-diretriz_1","title":"Resultados Esperados (diretriz)","text":"<p>Com separa\u00e7\u00e3o moderada (<code>class_sep\u22481.6</code>) e ru\u00eddo leve, espera-se acur\u00e1cia &gt; 0.85 no teste (varia com seed e hiperpar\u00e2metros). A PCA deve mostrar separa\u00e7\u00e3o razo\u00e1vel entre classes, ainda que com sobreposi\u00e7\u00e3o em regi\u00f5es mais dif\u00edceis.</p>"},{"location":"exercise3-MLP/main/#conformidade-e-reuso-ponto-extra","title":"Conformidade e Reuso (Ponto Extra)","text":"<ul> <li>O n\u00facleo da MLP (estrutura, forward/backward, atualiza\u00e7\u00e3o) mant\u00e9m a mesma forma do Exerc\u00edcio 2, apenas adaptando a camada de sa\u00edda para softmax e a perda para cross-entropy categ\u00f3rica. N\u00e3o foram utilizados frameworks de DL; somente NumPy para opera\u00e7\u00f5es matriciais.</li> </ul>"},{"location":"exercise3-MLP/main/#exercicio-4-mlp-mais-profunda-2-camadas-ocultas","title":"Exerc\u00edcio 4 \u2014 MLP Mais Profunda (\u2265 2 camadas ocultas)","text":"<p>Neste passo, repetimos o Exerc\u00edcio 3, por\u00e9m garantindo uma MLP mais profunda, com pelo menos duas camadas ocultas. O c\u00f3digo reutiliza a mesma implementa\u00e7\u00e3o do Exerc\u00edcio 3, alterando apenas a configura\u00e7\u00e3o de camadas ocultas via CLI.</p>"},{"location":"exercise3-MLP/main/#arquitetura_1","title":"Arquitetura","text":"<ul> <li>Entrada: 4 features; Sa\u00edda: 3 classes (softmax).</li> <li>Ocultas: pelo menos 2 camadas (ex.: <code>[64, 64, 32]</code>).</li> <li>Inicializa\u00e7\u00e3o: He nas ocultas e Xavier na sa\u00edda.</li> <li>Ativa\u00e7\u00f5es: ReLU nas ocultas e softmax na sa\u00edda.</li> <li>Perda: categorical cross-entropy; Otimizador: Gradiente Descendente.</li> </ul>"},{"location":"exercise3-MLP/main/#treinamento-e-avaliacao_1","title":"Treinamento e Avalia\u00e7\u00e3o","text":"<ul> <li>\u00c9pocas sugeridas: <code>450</code> (ajust\u00e1vel).</li> <li>M\u00e9trica: acur\u00e1cia no conjunto de teste.</li> <li>Artefatos gerados:</li> <li><code>training_loss_ex4.png</code> \u2014 curva da perda.</li> <li><code>pca_scatter_ex4.png</code> \u2014 PCA 2D colorida por classe prevista.</li> <li><code>confusion_matrix_ex4.png</code> \u2014 matriz de confus\u00e3o multiclasse.</li> <li><code>metrics_ex4.txt</code> \u2014 resumo de m\u00e9tricas e hiperpar\u00e2metros.</li> </ul>"},{"location":"exercise3-MLP/main/#como-executar-windows-powershell_2","title":"Como Executar (Windows PowerShell)","text":"<pre><code>.\\env\\Scripts\\python.exe docs\\exercise3-MLP\\mlp_ex4.py --samples 1500 --epochs 450 --lr 0.05 --hidden 64 64 32 --seed 42 --class_sep 1.6 --flip_y 0.01 --clusters 2 3 4\n</code></pre> <p>Em seguida: <pre><code>Get-Content docs\\exercise3-MLP\\metrics_ex4.txt\n</code></pre></p>"},{"location":"exercise3-MLP/main/#observacoes","title":"Observa\u00e7\u00f5es","text":"<ul> <li>A profundidade extra tende a capturar melhor a varia\u00e7\u00e3o entre os 2/3/4 clusters por classe, podendo melhorar a acur\u00e1cia. Entretanto, pode demandar mais \u00e9pocas ou tuning fino da taxa de aprendizado.</li> </ul>"},{"location":"projeto/main/","title":"Main","text":"<p>Aqui vai toda a documenta\u00e7\u00e3o do projeto, incluindo o que j\u00e1 foi feito e o que falta fazer.</p>"},{"location":"thisdocumentation/main/","title":"Main","text":""},{"location":"thisdocumentation/main/#pre-requisitos","title":"Pr\u00e9-requisitos","text":"<p>Antes de come\u00e7ar, certifique-se de que voc\u00ea possui os seguintes pr\u00e9-requisitos instalados em seu sistema:</p> <ul> <li>Git: Para clonar o reposit\u00f3rio.</li> </ul>"},{"location":"thisdocumentation/main/#instalando-o-python","title":"Instalando o Python","text":"LinuxmacOSWindows <p>Instale o Python 3.8 ou superior.</p> <pre><code>sudo apt install python3 python3-venv python3-pip\npython3 --version\n</code></pre> <p>Instale o Python 3.8 ou superior.</p> <pre><code>brew install python\npython3 --version\n</code></pre> <p>Instale o Python 3.13 ou superior. Baixe o instalador do site oficial do Python (https://www.python.org/downloads/) e execute-o. Certifique-se de marcar a op\u00e7\u00e3o \"Add Python to PATH\" durante a instala\u00e7\u00e3o.</p> <pre><code>python --version\n</code></pre>"},{"location":"thisdocumentation/main/#usage","title":"Usage","text":"<p>Para utilizar o c\u00f3digo deste reposit\u00f3rio, siga as instru\u00e7\u00f5es a seguir:</p> <p>Clone ou fork este reposit\u00f3rio:</p> <pre><code>git clone &lt;URL_DO_REPOSITORIO&gt;\n</code></pre> <p>Crie um ambiente virtual do Python:</p> Linux/macOSWindows <pre><code>python3 -m venv env\n</code></pre> <pre><code>python -m venv env\n</code></pre> <p>Ative o ambiente virtual (voc\u00ea deve fazer isso sempre que for executar algum script deste reposit\u00f3rio):</p> Linux/macOSWindows <pre><code>source ./env/bin/activate\n</code></pre> <pre><code>.\\env\\Scripts\\activate\n</code></pre> <p>Instale as depend\u00eancias com:</p> Linux/macOSWindows <pre><code>python3 -m pip install -r requirements.txt --upgrade\n</code></pre> <pre><code>python -m pip install -r requirements.txt --upgrade\n</code></pre>"},{"location":"thisdocumentation/main/#deployment","title":"Deployment","text":"<p>O material utiliza o mkdocs para gerar a documenta\u00e7\u00e3o. Para visualizar a documenta\u00e7\u00e3o, execute o comando:</p> <pre><code>mkdocs serve -o\n</code></pre> <p>Para subir ao GitHub Pages, execute o comando:</p> <pre><code>mkdocs gh-deploy\n</code></pre> <p>Esse reposit\u00f3rio possui um workflow do GitHub Actions que executa o comando <code>mkdocs gh-deploy</code> sempre que houver um push na branch <code>main</code>. Assim, n\u00e3o \u00e9 necess\u00e1rio executar esse comando manualmente. Toda vez que voc\u00ea fizer um push na branch <code>main</code>, a documenta\u00e7\u00e3o ser\u00e1 atualizada automaticamente no GitHub Pages.</p> <p>Aviso 1</p> <p>Para que o github actions funcione corretamente, \u00e9 necess\u00e1rio que o reposit\u00f3rio esteja configurado para que o bot <code>github-actions[bot]</code> tenha permiss\u00e3o de escrita. Voc\u00ea pode verificar isso nas configura\u00e7\u00f5es do reposit\u00f3rio, na se\u00e7\u00e3o \"Actions\" e depois em \"General\". Certifique-se de que a op\u00e7\u00e3o \"Workflow permissions\" esteja definida como \"Read and write permissions\".</p> <p></p> <p>Aviso 2</p> <p>Depois de publicar, caso n\u00e3o consiga acessar a p\u00e1gina, verifique se o github pages est\u00e1 configurado corretamente. V\u00e1 at\u00e9 as configura\u00e7\u00f5es do reposit\u00f3rio, na se\u00e7\u00e3o \"Pages\" e verifique se a branch <code>gh-pages</code> est\u00e1 selecionada como fonte. Se n\u00e3o estiver, selecione-a e salve as altera\u00e7\u00f5es.</p> <p></p> <p>Pay Attention</p> <p>No arquivo '<code>mkdocs.yml</code>, a se\u00e7\u00e3o <code>site_url</code> deve estar configurada corretamente para o seu reposit\u00f3rio. Por exemplo, se o seu reposit\u00f3rio estiver em <code>https://github.com/usuario/repositorio</code>, a se\u00e7\u00e3o <code>site_url</code> deve ser:</p> <pre><code>site_url: https://usuario.github.io/repositorio\n</code></pre> <p>Tamb\u00e9m, certifique-se de que a se\u00e7\u00e3o <code>repo_url</code> esteja configurada corretamente para o seu reposit\u00f3rio. Por exemplo:</p> <pre><code>repo_url: https://github.com/usuario/repositorio\n</code></pre>"}]}